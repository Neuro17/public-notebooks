{"metadata":{"name":"Spark - Kafka","user_save_timestamp":"1970-01-01T00:00:00.000Z","auto_save_timestamp":"1970-01-01T00:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"customLocalRepo":"/root/.ivy2","customRepos":null,"customDeps":["com.datastax.spark %% spark-cassandra-connector % 1.5.0-M1","org.apache.spark %% spark-streaming-kafka % _","org.apache.spark %% spark-graphx % _","- org.apache.spark % spark-core_2.10 % _","- org.apache.spark % spark-streaming_2.10 % _","- org.apache.hadoop % _ % _","- org.scala-lang % _ % _"],"customImports":null,"customArgs":null,"customSparkConf":{"spark.executor.memory":"1024m","spark.cassandra.connection.host":"127.0.0.1","spark.cores.max":"2","spark.executor.cores":"2","spark.master":"local[*]","spark.scheduler.mode":"FAIR"}},"cells":[{"metadata":{},"cell_type":"markdown","source":"# Earthquake <small>(like a duck)</small>"},{"metadata":{},"cell_type":"markdown","source":"## Process data (Spark) and Simulate stream (Spark -> Kafka)"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, we're going to focus on:\n1. reading and processing static data about earthquakes\n2. using data frame to shape them, organize them so we can do simple exploration\n3. use the time of occurence of the earthquakes to simulate a stream in a kafka topic\n4. read the data back using spark streamin to check its content looks good"},{"metadata":{},"cell_type":"markdown","source":"# Set Up"},{"metadata":{},"cell_type":"markdown","source":"### Data"},{"metadata":{},"cell_type":"markdown","source":"First, we set the folder where the cold dataset will be stored."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val folder = sys.env(\"DEMO_HOME\") + \"/datasets\"","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is the location of the online dataset: history of worldwide earthquakes provided by the <a href=\"https://www.ngdc.noaa.gov\" target=\"_blank\"><strong>National Centers for Environmental Information</strong></a>."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":":sh du -hs $folder/earthquakes.txt","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explore the local data"},{"metadata":{},"cell_type":"markdown","source":"### First touch"},{"metadata":{},"cell_type":"markdown","source":"Before moving forward, we need to have a sense of the data.\n\nSo we should first load the data in Spark and get our hands a little bit dirty, at least to understand the tabular structure."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val raw = sparkContext.textFile(folder+\"/earthquakes.txt\")","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What about the first line? We hope there is some inforamtion about the structure..."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val headers = raw.first","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like it. The first line defines values that should be available for the earthquakes we'll observe in the rest of the dataset. So it looks like we have an id, some time information, sinisters and so on."},{"metadata":{},"cell_type":"markdown","source":"We better save these columnns separated, we should need it soon."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val columns = headers.split(\"\\t\")","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create the strucutre "},{"metadata":{},"cell_type":"markdown","source":"For the future, we'd like to keep information well structure which makes evolution **simpler and harder** at the same time.\n\nOr should we say **safer but harder** at the same time?"},{"metadata":{},"cell_type":"markdown","source":"#### Diggin' the databook"},{"metadata":{},"cell_type":"markdown","source":"By chance, the **NOAA** did a good job and provides also a <a href=\"http://www.ngdc.noaa.gov/nndc/struts/results?&amp;t=101650&amp;s=225&amp;d=225\" target=\"_blank\"><strong>data book</strong></a> for the dataset. \n\nThis tells us that the file is actually a **TSV** but also provides the **semantic** for all columns. This gives us the key to determine efficiently what will be the best **datat type** for each colummn."},{"metadata":{},"cell_type":"markdown","source":"Since it can be painful to constantly switching between:\n* the databook main page\n* the column definition page (description, values, ...)\n* the structure definition (type in a class)\n* a data sample to check how to parse it (for TSV, the position in the row)\n\nWe're going to embed most of them in the following.\n\nIndeed, the following will create\n- an IFrame loading the data book\n- ah HTML form that will generate a `String` that will define the case class we'd like to define\n\n> This is also a toy but shows how we could use a mix of UI in plain Scala to generate artifacts simpler."},{"metadata":{},"cell_type":"markdown","source":"### Types and Parsers"},{"metadata":{},"cell_type":"markdown","source":"So good, we can now factorize the type and provide helpers for parsing."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"// Handles null/empty\nobject Readers {\n  def toInt(s:String) = s match {\n    case \"\" => None\n    case \"`NA`\" => None\n    case x => Some(x.toDouble.toInt)\n  }\n  def toDouble(s:String) = s match {\n    case \"\" => None\n    case \"`NA`\" => None\n    case x => Some(x.toDouble)\n  }\n  def toStringEx(s:String) = s match {\n    case \"\" => None\n    case \"`NA`\" => None\n    case x => Some(x)\n  }\n}\nimport Readers._\n\n/**\n * The date data\n */\ncase class EQDate(\n  YEAR:Option[Int] /*line(2) -- */ /*negative too*/, \n  MONTH:Option[Int] /*line(3) -- */ /*1-12*/, \n  HOUR:Option[Int] /*line(5) -- */ /*0-23*/, \n  DAY:Option[Int] /*line(4) -- */ /*1-31*/, \n  MINUTE:Option[Int] /*line(6) -- *//*0-59*/, \n  SECOND:Option[Int] /*line(7) -- */ /*0-59*/\n)\n/**\n * The date parser\n */\nobject EQDate {\n  def parse(line:Array[String]) = \n    EQDate(\n      toInt(line(2)),\n      toInt(line(3)),\n      toInt(line(5)),\n      toInt(line(4)),\n      toInt(line(6)),\n      toInt(line(7))\n    )\n}\n\n/**\n * The sinister data\n */\ncase class Counts(\n  MISSING:Option[Int] /*line(25) -- */ /*0 to 1100000*/, \n  MISSING_DESCRIPTION:Option[Int] /*line(26) -- */ /*0-4None to Very Many (~1001 or more deaths)*/, \n\n  DEATHS:Option[Int] /*line(23) -- */ /*0 to 1100000*/, \n  DEATHS_DESCRIPTION:Option[Int] /*line(24) -- */ /*0-4None to Very Many (~1001 or more deaths)*/, \n\n  INJURIES:Option[Int] /*line(27) -- */ /*0 to 1100000*/, \n  INJURIES_DESCRIPTION:Option[Int] /*line(28) -- */ /*0-4None to Very Many (~1001 or more deaths)*/, \n  \n  HOUSES_DAMAGED:Option[Int] /*line(33) -- */ /*0 to 1100000*/, \n  HOUSES_DAMAGED_DESCRIPTION:Option[Int] /*line(34) -- */ /*0-4None to Very Many (~1001 or more deaths)*/, \n\n  HOUSES_DESTROYED:Option[Int] /*line(31) -- */ /*0 to 1100000*/, \n  HOUSES_DESTROYED_DESCRIPTION:Option[Int] /*line(32) -- */ /*0-4None to Very Many (~1001 or more deaths)*/, \n\n  DAMAGE_MILLIONS_DOLLARS:Option[Double] /*line(29) -- */ /*to multiply by 1,000,000*/,\n  DAMAGE_DESCRIPTION:Option[Int] /*line(30) -- */ /*0-4None to EXTREME (~$25 million or more)*/\n)\n/**\n * The sinister parser\n */\nobject Counts {\n  def parse(line:Array[String]) = \n    Counts(\n      toInt(line(25)),\n      toInt(line(26)),\n      toInt(line(23)),\n      toInt(line(24)),\n      toInt(line(27)),\n      toInt(line(28)),\n      toInt(line(33)),\n      toInt(line(34)),\n      toInt(line(31)),\n      toInt(line(32)),\n      toDouble(line(29)),\n      toInt(line(30))\n    )\n}\n\n/**\n * The (Total) sinister data\n */\ncase class Totals(\n  TOTAL_INJURIES_DESCRIPTION:Option[Int] /*line(40) -- */ /*0-4None to Very Many (~1001 or more deaths)*/, \n  TOTAL_INJURIES:Option[Int] /*line(39) -- */ /*0 to 1100000*/, \n  \n  TOTAL_MISSING:Option[Int] /*line(37) -- */ /*0 to 1100000*/, \n  TOTAL_MISSING_DESCRIPTION:Option[Int] /*line(38) -- */ /*0-4None to Very Many (~1001 or more deaths)*/, \n  \n  TOTAL_DEATHS:Option[Int] /*line(35) -- */ /*0 to 1100000*/, \n  TOTAL_DEATHS_DESCRIPTION:Option[Int] /*line(36) -- */ /*0-4None to Very Many (~1001 or more deaths)*/, \n  \n  TOTAL_HOUSES_DESTROYED:Option[Int] /*line(43) -- */ /*0 to 1100000*/, \n  TOTAL_HOUSES_DESTROYED_DESCRIPTION:Option[Int] /*line(44) -- */ /*0-4None to Very Many (~1001 or more deaths)*/, \n\n  TOTAL_HOUSES_DAMAGED:Option[Int] /*line(45) -- */ /*0 to 1100000*/, \n  TOTAL_HOUSES_DAMAGED_DESCRIPTION:Option[Int] /*line(46) -- */ /*0-4None to Very Many (~1001 or more deaths)*/, \n  \n  TOTAL_DAMAGE_MILLIONS_DOLLARS:Option[Double] /*line(41) -- */ /*to multiply by 1,000,000*/, \n  TOTAL_DAMAGE_DESCRIPTION:Option[Int] /*line(42) -- */ /*0-4None to EXTREME (~$25 million or more)*/\n)\n/**\n * The (Total) sinister parser\n */\nobject Totals {\n  def parse(line:Array[String]) = \n    Totals(\n      toInt(line(40)),\n      toInt(line(39)),\n      toInt(line(37)),\n      toInt(line(38)),\n      toInt(line(35)),\n      toInt(line(36)),\n      toInt(line(43)),\n      toInt(line(44)),\n      toInt(line(45)),\n      toInt(line(46)),\n      toDouble(line(41)),\n      toInt(line(42))\n    )\n}\n\n/**\n * The Geolocation data\n */\ncase class Geo(\n  LATITUDE:Option[Double] /*line(20) -- */ /*-90 to +90*/, \n  LONGITUDE:Option[Double] /*line(21) -- */ /*-180 to +180*/, \n  COUNTRY:Option[String] /*line(17) -- */, \n  STATE:Option[String] /*line(18) -- */ /*The two-letter State or Province abbreviation*/, \n  REGION_CODE:Option[Int] /*line(22) -- */ /*10 = Central, Western and S. Africa; 15 = Northern Africa*/, \n  LOCATION_NAME:Option[String] /*line(19) -- */ /*The Country, State, Province or City where the Earthquake occurred (For example enter: USA or California or San Francisco)*/ \n)\n/**\n * The Geolocation parser\n */\nobject Geo {\n  def parse(line:Array[String]) = \n    Geo(\n      toDouble(line(20)),\n      toDouble(line(21)),\n      toStringEx(line(17)),\n      toStringEx(line(18)),\n      toInt(line(22)),\n      toStringEx(line(19))      \n    )\n}\n\n/**\n * The imoprtance data\n */\ncase class EQCharacteristics(\n  EQ_MAG_MFA:Option[Double] /*line(14) -- */ /* 0.0 to 9.9*/, \n  EQ_MAG_ML:Option[Double] /*line(13) -- */ /* 0.0 to 9.9*/, \n  EQ_MAG_MB:Option[Double] /*line(12) -- */ /* 0.0 to 9.9*/, \n  EQ_MAG_MW:Option[Double] /*line(10) -- */ /* 0.0 to 9.9*/, \n  EQ_MAG_UNK:Option[Double] /*line(15) -- */ /* 0.0 to 9.9*/, \n  EQ_MAG_MS:Option[Double] /*line(11) -- */ /* 0.0 to 9.9*/,\n  \n  EQ_PRIMARY:Option[String] /*line(9) -- ???*/ \n)\n/**\n * The imoprtance parser\n */\nobject EQCharacteristics {\n  def parse(line:Array[String]) = \n    EQCharacteristics(\n      toDouble(line(14)),\n      toDouble(line(13)),\n      toDouble(line(12)),\n      toDouble(line(10)),\n      toDouble(line(15)),\n      toDouble(line(11)),\n      toStringEx(line(9))\n    )\n}\n\n\n/**\n * The data model\n */\ncase class DataModel(\n  I_D:Option[Int] /*line(0) -- */, \n  \n  FLAG_TSUNAMI:Boolean /*line(1) -- */ /*Tsu or empty Option[string]*/, \n  INTENSITY:Option[Int] /*line(16) -- */ /*1 to 12*/, \n  FOCAL_DEPTH:Option[Int] /*line(8) -- */ /*0 to 700 km*/, \n\n  date:EQDate,\n  counts:Counts,\n  totals:Totals,\n  geo:Geo,\n  eq:EQCharacteristics\n)\n/**\n * The data model parser that TAKES CARE OF LEADING TABS (removed by split -_-)\n */\nobject DataModel {\n  def parse(s:String) = {\n    val r = s.replaceAll(\"\\t\\t\", \"\\t`NA`\\t\")\n    val r2 = if (r.last == '\\t') r+\"`NA`\" else r\n\n    val line = r2.split(\"\\t\").map(_.trim)\n    DataModel(\n      toInt(line(0)),\n      line(1).nonEmpty,\n      toInt(line(16)),\n      toInt(line(8)),\n      \n      EQDate.parse(line),\n      Counts.parse(line),\n      Totals.parse(line),\n      Geo.parse(line),\n      EQCharacteristics.parse(line)\n    )\n  }\n}","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation with DataFrame"},{"metadata":{},"cell_type":"markdown","source":"#### Setup"},{"metadata":{},"cell_type":"markdown","source":"DataFrame needs sql, hence we put sql in the middle of the place."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val sqlContext = new org.apache.spark.sql.SQLContext(sparkContext)\nimport sqlContext.implicits._","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We're then ready for it."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val observations = raw.filter(_ != headers)\n                      .map(DataModel.parse)\n                      .toDF()","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Organize the data: **Date**"},{"metadata":{},"cell_type":"markdown","source":"Is the date worth considering for organizing the data?"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import org.apache.spark.sql.functions._\nval countByYear = observations.groupBy(\"date.YEAR\").agg(count($\"I_D\"))\n                               .map(row => (row.getAs[Int](\"YEAR\"), row.getAs[Long](\"count(I_D)\"))).collect","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay, threre are quite some years involved here... what is the full picture over here.\n\nFor this, we can use directly the widgets to do that but it then requires us to pass 2 parameters:\n1. the number of points we'd like to consider (all of them)\n2. the fields of the data structure to be taken as `x` and `y` (it's a **tuple**, hence the `_1` and `_2`)."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"widgets.LineChart(countByYear, maxPoints=countByYear.size, fields=Some((\"_1\", \"_2\")))","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sounds like there is a structure in the date pattern, however we already have an idea which and why? Don't we?"},{"metadata":{},"cell_type":"markdown","source":"#### Organize the data: **Geolocation**"},{"metadata":{},"cell_type":"markdown","source":"It should be helpful to have a facet of the data also showing the spread of the data on the earth.\n\nAll in all, we're talking about earthquakes, so the earth itself should play a role."},{"metadata":{},"cell_type":"markdown","source":"First, we rearrange the data by binning the earthquakes in a squared grid of 100x100 cells.\n\nThis is very simple but fine by now."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val geoBinObs = observations.withColumn(\"latBin\", floor((($\"geo.LATITUDE\" + 90) / 180) * 100))\n                            .withColumn(\"lonBin\", floor((($\"geo.LONGITUDE\" + 180) / 360) * 100))","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data View"},{"metadata":{},"cell_type":"markdown","source":"In this section, we'll just do one thing, that is, create a view of the data showing the importance and the gravity of the earthquakes.\n\nWe'll use a very simple model where:\n* importance is represented by the sum of total deaths\n* the gravity is represented by the mean of total deaths"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val geoSeverityPerYear = geoBinObs.groupBy(\"date.YEAR\", \"latBin\", \"lonBin\")\n                                  .agg(mean(\"totals.TOTAL_DEATHS\").as(\"avg\"), \n                                       sum(\"totals.TOTAL_DEATHS\").as(\"sum\"))\n                                  .orderBy(\"YEAR\")","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note** that we've organized by year, for what's coming next."},{"metadata":{},"cell_type":"markdown","source":"However, since we're gonna use this version several times, it's time to put it in the cache."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"geoSeverityPerYear.cache()\ngeoSeverityPerYear.rdd.name = \"Geo Severity Per Year\"","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Produce to Kafka"},{"metadata":{},"cell_type":"markdown","source":"### Intro"},{"metadata":{},"cell_type":"markdown","source":"This data that we've got at once is pretty interesting to show how this could have happen in real (fast forwarded a lot) life.\n\nBeceuse, of course, earthquakes is a stream of events.\n\nSo what about simulating this by producing these data in a queue like it would have been gathered by a collection service.\n\nHere, we'll do soemthing very weird but funny with spark fir this purpose"},{"metadata":{},"cell_type":"markdown","source":"We'll simulate the time moving on, so that we need to know the range of dates we're interested in.\n\nThis will help us creating and maintaining a publishing rate that looks like real (but fast)."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val Seq(minDate, maxDate) = geoSeverityPerYear.agg(min(\"YEAR\"), max(\"YEAR\"))\n                                                .collect\n                                                .head\n                                                .toSeq\n                                                  .map(_.toString.toInt)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A bit of computations to know the time speed we need."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import scala.concurrent.duration._\nval howLong = 1 minutes\nval yearsPerMillisecond = (maxDate - minDate) / howLong.toMillis.toDouble\nval millisecondsPerYear = 1 / yearsPerMillisecond","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Simulated time Production"},{"metadata":{},"cell_type":"markdown","source":"Okay, here we are. We're now reading to push the earthquakes in the kafka queue."},{"metadata":{"trusted":true,"input_collapsed":false,"output_stream_collapsed":true,"collapsed":false},"cell_type":"code","source":"def scope = new java.io.Serializable {\n  import org.apache.spark.broadcast.Broadcast\n  \n  val md = minDate.abs\n  val ms = millisecondsPerYear\n  val startedAt = System.currentTimeMillis\n\n  geoSeverityPerYear.rdd.coalesce(1, true).foreach { row =>\n    // NOT SO GOOD BUT OKAY HERE\n    // each message create a producer o?\n    import java.util.Properties\n    import org.apache.kafka.clients.producer.{KafkaProducer,ProducerConfig,ProducerRecord}\n    val props = new Properties()\n    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092,localhost:9093\")\n    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringSerializer\")\n    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringSerializer\")\n    val producer = new KafkaProducer[String, String](props)  \n\n\n    // RETRIEVING DATE AND BUILDING MESSAGE\n    val year = row.getAs[Int](\"YEAR\")\n    val msg = row.toSeq.mkString(\",\")\n\n    // COMPUTING THE DELAY\n    // based on the time speed\n    val shoulStartAt = ((year + md) * ms) + startedAt\n    val now = System.currentTimeMillis\n    val delay = shoulStartAt - now //assuming machines synchronized\n\n\n    // WAIT UNTIL ITS TIME ARRIVES\n    if (delay > 0) { \n      Thread.sleep(delay.toLong) // not really true => due to spark scheduling\n    }\n\n\n    // PRODUCING\n    producer.send(new ProducerRecord[String, String](\"demo\", year.toString, msg))\n  }\n}\n\n\n/**\n * Let's launch this in a Future so we don't block the notebook :-]\n */\nimport scala.concurrent.Future\nimport scala.concurrent.ExecutionContext.Implicits.global\nFuture { scope }","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check Kafka from Spark Streaming"},{"metadata":{},"cell_type":"markdown","source":"## Setup"},{"metadata":{},"cell_type":"markdown","source":"What's cool with notebook is that you can start, restart, rerererererestart you're test live, but then you need to also restart your streaming context (yet keeping your spark context live).\n\n**Do you see why?**"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import org.apache.spark.streaming.Seconds\nimport org.apache.spark.streaming.StreamingContext\nStreamingContext.getActive foreach (_.stop(false))","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great, fresh new environement, so we can move on and create a new streaming context and prepare the kafka connection."},{"metadata":{"trusted":true,"input_collapsed":false,"output_stream_collapsed":true,"collapsed":false},"cell_type":"code","source":"val ssc = new StreamingContext(sc, Seconds(2))\nval brokers = \"localhost:9092,localhost:9093\"\nval topics = Set(\"demo\")\nval kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers, \"auto.offset.reset\" -> \"largest\" /*\"smallest\"*/)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Connect to Kafka"},{"metadata":{},"cell_type":"markdown","source":"It's finally time to connect to kafka."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import org.apache.spark.streaming.kafka.KafkaUtils\nimport org.apache.spark.streaming.Time\nimport kafka.serializer.StringDecoder\n\ntype SD = StringDecoder\nval eqStream = KafkaUtils.createDirectStream[String, String, SD, SD](ssc, kafkaParams, topics)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DONE!"},{"metadata":{},"cell_type":"markdown","source":"Now that we're connected to the kafka queue are ready to receive batches of `2 seconds` of data, we can also keep an eye on what load we have there.\n\nThe following shows how you can do it in a reactive list (without checking the spark UI thus)."},{"metadata":{},"cell_type":"markdown","source":"## Data Load"},{"metadata":{"trusted":true,"input_collapsed":false,"output_stream_collapsed":true,"collapsed":false},"cell_type":"code","source":"val counts = ul(15)\neqStream.foreachRDD(rdd => counts.append(\"\"+rdd.count))\ncounts","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"See nothing? Of course not, wait for a sec ;-)."},{"metadata":{},"cell_type":"markdown","source":"## Read Parse Show"},{"metadata":{},"cell_type":"markdown","source":"In this section, we're gonna read the incoming stream of earthquake that was streamed in as string previously.\n\nSince we're simply checking what's going on here, we'll be simply display those events in the geo map as circles and colors to represent the severity of the places along time."},{"metadata":{},"cell_type":"markdown","source":"First we compute some boundary statistics to create some scales after."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val Seq(minSum, maxSum, minAvg, maxAvg) = geoSeverityPerYear.agg(min($\"sum\"), max($\"sum\"), \n                                                                 min($\"avg\"), max($\"avg\")\n                                                             )\n                                                              .first\n                                                              .toSeq\n                                                                .map(_.toString.toDouble)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following isn't necessary but I kindo like encapsulate stuffs in classes, so that using widgets, the syntax shows semantic (field names)."},{"metadata":{"trusted":true,"input_collapsed":false,"output_stream_collapsed":true,"collapsed":false},"cell_type":"code","source":"object P extends Serializable { // cool trick to create package in a REPL ^^\n  val ms = minSum // thanks to these we also clean the closure context \\o/\n  val Ms = maxSum\n  val ma = minAvg\n  val Ma = maxAvg\n  \n  case class GeoShake(lat:Double, lon:Double, importance:Double, sad:String) extends Serializable\n\n  object GeoShake extends Serializable {\n    \n    // A red-ish scale brewer\n    val colors = List(\n      \"#fff5f0\",\n      \"#fee0d2\",\n      \"#fcbba1\",\n      \"#fc9272\",\n      \"#fb6a4a\",\n      \"#ef3b2c\",\n      \"#cb181d\",\n      \"#99000d\"\n    )\n    \n    def apply(s:String):GeoShake = {\n      // reading values out of the line, with damn nulls in it\n      val List(_, latBin, lonBin, avg, sum) = s.split(\",\").toList\n                                                          .map(s => Option(s)\n                                                                      .filter(_ != \"null\")\n                                                                      .map(_.toDouble)\n                                                              )\n      \n      // replacee the event in the center of the bin\n      val lat = latBin.getOrElse(50d) * (180d/100) - 90 + (180d/100)/2\n      val lon = lonBin.getOrElse(50d) * (360d/100) - 180 + (360d/100)/2\n      \n      // compute the circle radius\n      val importance = sum.map(sum => (sum-ms).toDouble/(Ms-ms)).getOrElse(0d) * 30\n      \n      // compute the sad measure as color\n      val sad = colors(avg.map(avg => (avg-ma)/(Ma-ma)).getOrElse(0d).toInt)\n      \n      /** DEBUG Parse\n      org.slf4j.LoggerFactory.getLogger(\"PARSE\").info(s\"\"\"\n      ----------------------------------------------------------------------\n        line:   '$s'\n        parsed: '$latBin', '$lonBin', '$avg', '$sum''\n        data:   '$lat', '$lon', '$importance', '$sad'\n      \"\"\")\n      */\n      new GeoShake(lat, \n                   lon,\n                   importance,\n                   sad\n                  )\n\n    }\n  }\n}","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sounds like a plan has been made up! What about combining the whole #!"},{"metadata":{},"cell_type":"markdown","source":"In the cell below, we create a reactive geo plot which takes `GeoShake` instances in a `Seq`.\n\nThen we consume each `RDD` convert in these instances, followed by an update of the stream behind the geo plot using `applyOn`.\n\nBTW, we only show 1000 points."},{"metadata":{"trusted":true,"input_collapsed":false,"output_stream_collapsed":true,"collapsed":false},"cell_type":"code","source":"val geo = widgets.GeoPointsChart(Seq(new P.GeoShake(0,0,minSum,P.GeoShake.colors.head)), \n                                 maxPoints=1000, \n                                 latLonFields=Some((\"lat\", \"lon\")),\n                                 rField=Some((\"importance\")),\n                                 colorField=Some((\"sad\")),\n                                 sizes=(800,800)\n                                )\n\nvar globalData = List.empty[P.GeoShake]\n\nval scope = new java.io.Serializable {\n  val s = eqStream\n  @transient val g = geo\n  val parse = (s:String) => P.GeoShake.apply(s)\n  s.foreachRDD{ rdd => \n    val data = rdd.map(_._2).collect.map(parse).toList\n    globalData = globalData:::data\n    //org.slf4j.LoggerFactory.getLogger(\"GEO\").info(data.mkString(\" ; \"))\n    g.applyOn(globalData.takeRight(1000))\n  }\n}\ngeo","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Start Streaming"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"ssc.start","outputs":[]}],"nbformat":4}