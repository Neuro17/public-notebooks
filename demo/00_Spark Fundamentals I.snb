{"metadata":{"name":"Spark Fundamentals I","user_save_timestamp":"1970-01-01T01:00:00.000Z","auto_save_timestamp":"1970-01-01T01:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"customLocalRepo":null,"customRepos":null,"customDeps":null,"customImports":null,"customArgs":null,"customSparkConf":null},"cells":[{"metadata":{"trusted":true,"input_collapsed":true,"output_stream_collapsed":true,"collapsed":true},"cell_type":"code","source":":javascript\nrequire([\"jquery\", \"underscore\"], function(j, _) {\n  j(\".toc\").remove();\n  var toc = j(document.createElement(\"div\"));\n  toc.attr(\"id\", \"toc\")\n     .addClass(\"toc\")\n     .css(\"position\", \"fixed\")\n     .css(\"top\", \"15%\")\n     .css(\"width\", \"8%\")\n     .addClass(\"panel\").addClass(\"panel-info\");\n  \n  var tocHeading = j(document.createElement(\"div\")); \n  tocHeading.addClass(\"panel-heading\")\n            .appendTo(toc);\n  var tocTitle = j(document.createElement(\"h5\"));\n  tocTitle.text(\"Table of Content\")\n          .appendTo(tocHeading);\n  \n  var tocBody = j(document.createElement(\"div\")); \n  tocBody.addClass(\"panel-body\")\n         .appendTo(toc);\n  \n  var bodyContent = j(document.createElement(\"div\"));\n  bodyContent.css(\"max-height\", \"400px\")\n             .css(\"overflow\", \"auto\")\n             .css(\"max-width\", \"400px\")\n             .appendTo(tocBody);\n  \n  var notebook = j(\"#notebook\");\n\n  var current = 0;\n  var ul, li;\n  var container = bodyContent;\n  j(notebook).find(\"h1,h2,h3\").each(function(i, e) {\n    var lvl = j(e).prop(\"tagName\").substring(1)*1;\n    if (current != lvl) {\n      if (Math.min(current, lvl) == current) {\n        current = lvl;\n        var addUl = function(l) {\n          var c = container;\n          container = j(document.createElement(\"ul\")).css(\"padding-left\", \"15px\");\n          container.addClass(\"lvl\"+(current+l));\n          c.append(container);\n          if (l != 0) addUl(l-1);      \n        }\n        addUl(lvl-current);\n      } else {\n        container = container.parents(\".lvl\"+lvl);\n        current = lvl;\n      }\n    }\n    li = j(document.createElement(\"li\")).html(j(e).html());\n    var lk = li.find(\".anchor-link\");\n    var href = lk.attr(\"href\");\n    lk.remove();\n    var a = j(document.createElement(\"a\")).html(li.html()).attr(\"href\", href);\n    li.empty();\n    li.append(a)\n    li.css(\"white-space\", \"nowrap\")\n    container.append(li);\n  });\n\n  toc.on(\n    {\n       \"mouseenter\": function() {j(this).css(\"width\", \"20%\");},\n       \"mouseleave\": function() {j(this).css(\"width\", \"8%\");},\n    }\n  );\n  \n  toc.appendTo(j(\"body\"));\n  j('#toc').affix();\n});","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":true,"collapsed":true},"cell_type":"code","source":":javascript\nvar f = function(){\n  var b = $$(\".badge\");\n  b .parent()\n    .addClass(\"alert\")\n    .addClass(\"alert-info\");\n  setTimeout(f, 1000);\n};\nsetTimeout(f, 1000);","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RDD and SparkContext"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, you'll see:\n\n* what is `SparkContext`\n* what is an `RDD` in Scala\n* the lazy definition of computation\n* the data/spark partitions\n* some spark functions for transformations\n* some spark functions for actions\n\nFor that, we'll:\n\n* read a text dataset\n* parse it as Json\n* convert to a domain type\n* manipulate the data into categories\n* plot some views on the categories"},{"metadata":{},"cell_type":"markdown","source":"## Set Up"},{"metadata":{},"cell_type":"markdown","source":"First, we download some data to our local folder.\n\nThis data is pulled from s3 using https and was initially published by [Paco Nathan](http://www.slideshare.net/pacoid/microservices-containers-and-machine-learning-50862677). So, thanks a lot @pacoid!\n\nThe dataset is a json export of the June 2015's Spark mailing list."},{"metadata":{},"cell_type":"markdown","source":"**Note:** the data is local, but Spark will read it _as_ distributed"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val dataFile = sys.env(\"DEMO_HOME\")+\"/datasets/mails-parsed.json\"","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First Step: read the data"},{"metadata":{},"cell_type":"markdown","source":"### Driver: SparkContext"},{"metadata":{},"cell_type":"markdown","source":"When you need to read some data from a distributed (or not) datasource in Spark, you always use an instance of `org.apache.spark.SparkContext` -- or one of its wrappers.\n\nThis is because this object is actually the puppet master of Spark, it holds the whole configuration of the job but also information about the runtime.\n\nIt's sometimes called the driver, because it is created on the machine running the application and will drive the job for you on the cluster (this include sending, tracking and so on the distributed operations).\n\nIn this notebook, the `SparkContext` instance is created for you using the metadata stored in this notebook and is named `sparkContext` and has an alias `sc`.\n\nThe configuration is the following:\n```\n{\n  \"spark.app.name\" : \"Spark Concepts - RDD and SparkContext\", #added by the spark notebook for you\n  \"spark.master\": \"local[*]\"\n}\n```\n\nAs you can see the `spark.master` configuration says that the cluster is *local* and uses *all* CPUs available."},{"metadata":{},"cell_type":"markdown","source":"### textFile"},{"metadata":{},"cell_type":"markdown","source":"So, we have a text file on our local file system that we'd like to read and explore. <span style=\"font-size:50%\">Do you remember how painful it is in Java?</span>\n\nIn Scala, you can run `scala.io.Source.fromFile(...).getLines` right? In Spark, it's gonna be as easy, and even easier, since we'll just call the function `textFile` on `sparkContext`.\n\nI say, it's easier because `textFile` can read distributed files like on `HDFS`, `S3`, ... just by providing the path and (almost) nothing else!"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val asText = sparkContext.textFile(dataFile)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### How is this working?"},{"metadata":{},"cell_type":"markdown","source":"So what's going on here? We asked to read a file and it returned immediately!\n\nActually, the `sparkContext` can do two main things with data:\n\n* defining some <span style=\"color:red\">transformations</span> which are **lazy** manipulations of the data\n* running <span style=\"color:red\">actions</span> on the cluster which are, in a nutshell, operations that runs the defined transformations and returns the result\n\nIn the context of Big Data, or to be more precise, Distributed Computing, reading a dataset cannot be an action for which we can expect a result of course. It can only be lazy and thus `textFile` is a **transformation**.\n\n**Note:** `textFile` could have return a cursor/iterator but then we would have had the responsibility to deal with disconnections and failures. Lazy computation takes that off our hands."},{"metadata":{},"cell_type":"markdown","source":"#### What has been done?"},{"metadata":{},"cell_type":"markdown","source":"`textFile` is a transformation, which means that it defines some work to be done on the data.\n\nIts behavior is pretty simple, as a user we can think it will provide a container of `String`s, each `String` being a line in the text document.\n\nBut let's take a look at the result **type**: `RDD[String]`.\n\nLooking deeper and we see that the real class is actually:\n\n* `MapPartitionsRDD[String]` hence there are partitions going on, see below\n* `HadoopRDD[String]` hence it relies on distributed data APIs coming from HDFS."},{"metadata":{},"cell_type":"markdown","source":"We can take a look at something call the **DAG** (wait for it, in the next session)."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"asText.toDebugString","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Partitions"},{"metadata":{},"cell_type":"markdown","source":"As we already mentioned, although our dataset is read from the local file system, Spark allows us (almost for free) to deal with our data in a **parallel** way by splitting it behind the curtain."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"asText.partitions.size","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Transformation means no tasks"},{"metadata":{},"cell_type":"markdown","source":"In case you don't believe me, there is a way to check that nothing ran on the cluster: the super powerful and handy [**Spark UI**](http://localhost:4040)"},{"metadata":{},"cell_type":"markdown","source":"### Take(3)"},{"metadata":{},"cell_type":"markdown","source":"When dealing with data, the first thing that we do is to *take*, even a quick, look at the data.\n\nTo do that locally, there is of course the `head` command. However, here, we're talking about data spread in the cluster, data that cannot be accessed easily and of course ssh'ing to the nodes to access parts independently if foolish.\n\nLet's use Spark for that!\n\n`RDD` has this **action** function, `take(nb:Int)`. Since `take` is an action, we have the it:\n\n* runs **tasks** on the cluster\n* returns the result to the **driver**. *WARN:* take care at your driver's memory then!"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"asText.take(3)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### And Action"},{"metadata":{},"cell_type":"markdown","source":"So the result is coming fast, of course the data is local, however it's not as fast as `head` even locally.\n\nWhy that, it's because it's an action, and thus the driver needs to communicate tasks the cluster. Which means that the driver needs to send the whole computations to the worker nodes that will apply the logic on their data. This has a cost.\n\nLet's check the [Spark UI](http://localhost:4040) that a job ran and has one task."},{"metadata":{},"cell_type":"markdown","source":"## Json"},{"metadata":{},"cell_type":"markdown","source":"Now that, we are sure that we are reading Json strings, we can move on and parse them into more convenient objects.\n\nHere we're gonna use the `Json.parse` function from [Play 2](https://www.playframework.com/documentation/2.4.x/api/scala/index.html#play.api.libs.json.Json$) that reads `String`s into `JsValue`s (cast into `JsObject` for brevity)."},{"metadata":{},"cell_type":"markdown","source":"So, we want to *transform* each element in the `RDD` of `String` into a `JsObject`.\n\nThat's exactly the behavior of **`map`**! What comes next is then trivial."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import play.api.libs.json._\nval asJson = asText.map { x => \n               org.apache.log4j.Logger.getLogger(\"comp\").info(\"reading at \" + new java.util.Date())\n               Json.parse(x).as[JsObject]\n            }","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Note:\n>\n> The function we gave to map is also logging when it gets executed.\n>\n> Keep that in mind, it will help us understand how Spark is dealing with lazy computation later on"},{"metadata":{},"cell_type":"markdown","source":"Again, the result was immediate and the type didn't changed much because `map` is also a **transformation**, hence it is lazy and will **wait for an action to be called before executing its logic**.\n\nWe can see that by checking the type then `RDD[JsObject]`: `RDD`s are lazy, they are just **definitions**."},{"metadata":{},"cell_type":"markdown","source":"To apply quickly the map and check that it does what we expect, we can then call an action.\n\nWe'll reuse `take` since that's one of the cheapest actions provided by `RDD`s."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val JsObject(a) = asJson.take(1).head","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Email"},{"metadata":{},"cell_type":"markdown","source":"By printing the json example above, we can also take a look at the content of the objects. Based on which we can create a Scala type that will help us manipulating the objects in the future.\n\nLet's crate the `Email` type then."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import java.util.Date\nobject p extends java.io.Serializable {\n\n  case class Email(\n    id:String, \n    sender:String, \n    date:Date, \n    subject:String, \n    text:String, \n    next_url:String, \n    prev_thread:String, \n    next_thread:String\n  ) extends java.io.Serializable \n  \n  object Email extends java.io.Serializable {\n    import play.api.libs.json._\n    def fromJson(json: JsObject) = Email(\n      (json \\ \"id\").as[String], \n      (json \\ \"sender\").as[String], \n      (json \\ \"date\").as[Date], \n      (json \\ \"subject\").as[String], \n      (json \\ \"text\").as[String], \n      (json \\ \"next_url\").as[String], \n      (json \\ \"prev_thread\").as[String], \n      (json \\ \"next_thread\").as[String]    \n    )\n  }\n}","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, reading the _distributed_ data as `Email` from `JsObject`? It's also a `map` of course."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val asEmail = asJson map p.Email.fromJson","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no surprise now that the result came in milliseconds and the type of the `RDD`'s elements is `Email`."},{"metadata":{},"cell_type":"markdown","source":"## Interlude: `RDD` parent and dependencies"},{"metadata":{},"cell_type":"markdown","source":"We're going to show something here on which we'll come back in further sections.\n\nBut it's interesting to look on what are these lazy computations, `RDD`s, relying on.\n\nThere are two fields privately defined on `RDD` that can tell us even more about this:\n\n* `firstParent` (it can have several): points to the previous lazy computation that needs to be executed before the current one\n* `getDependencies`: gives information on how the **data** depends on the result of previous computations"},{"metadata":{"trusted":true,"input_collapsed":false,"output_stream_collapsed":true,"collapsed":false},"cell_type":"code","source":"implicit val cc = implicitly[scala.reflect.ClassTag[p.Email]]\nval c = asEmail.getClass\nval parent = c.getMethods.find(_.getName == \"firstParent\").get.invoke(asEmail, cc)\nval deps = c.getMethods.find(_.getName == \"getDependencies\").get.invoke(asEmail)\n<ul>\n  <li>The parent is asJson? {\"\" + (parent == asJson)}</li>\n  <li>The dependency of current data on previous result is: {deps.toString}</li>\n</ul>","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we see that:\n\n* the firstParent is actually the `asJson` (`RDD[JsObject]`) \n* the dependencies between the data in `asEmail` has a **one to one** dependency on the data in `asJson`. Of course, because we only need one `JsObect` to create one `Email`."},{"metadata":{},"cell_type":"markdown","source":"# Manual classification"},{"metadata":{},"cell_type":"markdown","source":"What we'll be doing for the remaining of this example is massaging the emails, very rough manipulations like manual categorization of the mails.\n\nIn the following, we'll flag each email with a bunch of categories to which it seems to correspond based on its content. <span style=\"font-size:50%\">(For the ML freaks, I know, I could have try to run an LDA for instance and then make an interpretion of the classes... but nah.)</span>."},{"metadata":{},"cell_type":"markdown","source":"This can be done by mapping (`map` is very helpful huh) each `Email` to :\n* its list of categories \n* and itself.\n\nTo do that we:\n* create a map of categories and tokens, \n* we check if the subject or the text of an email contains one of those tokens, \n* if it does, it will be associated to it."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val cats = asEmail.map { email =>\n  val allCategories = Map(\n    \"graphx\"        -> List(\"graphx\"),\n    \"sql\"           -> List(\"sql\",\"jdbc\"),\n    \"dataframe\"     -> List(\"dataframe\", \" df\"),\n    \"streaming\"     -> List(\"streaming\",\"kafka\",\"dstream\", \"flume\"),\n    \"core\"          -> List(\" core\", \"rdd\"),\n    \"adam\"          -> List(\"adam\"),\n    \"thunder\"       -> List(\"thunder\"),\n    \"tachyon\"       -> List(\"tachyon\"),\n    \"mllib\"         -> List(\"mllib\", \"kmeans\", \"k-means\", \"tf-idf\", \"random forest\", \" rf \", \" lda \", \" nlp \", \"knn\"),\n    \"pyspark\"       -> List(\"pyspark\",\"python\"),\n    \"sparkr\"        -> List(\"sparkr\",\" r \"),\n    \"connectors\"    -> List(\"cassandra\", \"cql\", \"couchdb\", \"kinesis\", \"neo4j\", \"riak\", \"flume\"),\n    \"cluster\"       -> List(\"yarn\", \"mesos\"),\n    \"notebook\"      -> List(\"notebook\"),\n    \"question\"      -> List(\"question\", \"?\", \" info\"),\n    \"problem\"       -> List(\"exception\", \"error\", \"issue\", \"problem\", \"doesn't\", \"does not\", \"not working\", \"unable\"),\n    \"serialization\" -> List(\"serializable\", \"serialization\", \"serializability\"),\n    \"community\"     -> List(\"unsubscribe\", \"subscribe\", \"announce\"),\n    \"build\"         -> List(\"maven\", \"sbt\", \"idea\", \"eclipse\", \" ant \", \"graddle\"),\n    \"performance\"   -> List(\"perf\", \"performance\")\n  )\n  val categories = for {\n    (cat, tokens) <- allCategories.toList if tokens.exists { token => \n                                                              (email.subject.toLowerCase contains token) || \n                                                              (email.text.toLowerCase contains token)\n                                                           }\n  } yield cat\n\n  categories.toList -> email\n}","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great, here we used a bit of handy Scala mechanisms to process the classification. Mainly in the _for-comprehension_ (the `for {}`).\n\nA _for-comprehension_ is not \"really\" a loop, it's a construction that chains generators (<small style=\"font-size=30%\">monads</small>), and the generated elements can be conditioned by a guard.\n\nHere we did the following:\n\n* for each email\n* _loops_ over all categories considered as a list (that is a list of pairs `(String, List[String])`) \n* pattern matches / extracts the elements in a pair (extract values) `(cat, tokens)`\n* filters out the category if no token is found in the subject and the body of the email. For this, we used the Scala `exists` function on `List`.\n* yields all categories that aren't filtered out."},{"metadata":{},"cell_type":"markdown","source":"This is exactly the same as:\n```scala\nallCategories.filter { case (cat, tokens) =>\n  tokens.exists { token => \n            (email.subject.toLowerCase contains token) || \n            (email.text.toLowerCase contains token)\n         }\n}.map { case (cat, tokens) =>\n  cat\n}\n```\nor\n```scala\nallCategories.filter { case (cat, tokens) =>\n  tokens.exists { token => \n            (email.subject.toLowerCase contains token) || \n            (email.text.toLowerCase contains token)\n         }\n}.map(_._1)\n```\nor\n```scala\nallCategories.filter(_._2.exists { token => \n                      (email.subject.toLowerCase contains token) || \n                      (email.text.toLowerCase contains token)\n                   })\n             .map(_._1)\n```\nor even :-/\n```scala\nallCategories.collect { \n  case (cat, tokens) if tokens.exists { token => \n                        (email.subject.toLowerCase contains token) || \n                        (email.text.toLowerCase contains token)\n                     } =>\n   cat\n}\n```"},{"metadata":{},"cell_type":"markdown","source":"Enough now!\n\nIt's worth noting that the element type is now `(List[String], Email)`, see below for some examples:"},{"metadata":{"trusted":true,"input_collapsed":false,"output_stream_collapsed":true,"collapsed":false},"cell_type":"code","source":"cats.take(3)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting some lenghts"},{"metadata":{},"cell_type":"markdown","source":"So far, we used few methods provided by `RDD` and `SparkContext`, because we only read a string dataset and, in fact, `map` can do a lot (remember that MapReduce is **only** composed of _\"map\"_ and _\"reduce\"_ finally)."},{"metadata":{},"cell_type":"markdown","source":"Here we're going to use another helpful one, **`filter`** which has the exact same semantic than in pure Scala.\n\nThere are other helpful methods in Spark of course, and we recommend you to check the API for that, but we'll meet most of them during the course - so no rush.\n\nNote that, by introducing a native support of `filter`, we're already farer than the raw MapReduce paradigm."},{"metadata":{},"cell_type":"markdown","source":"### Plot the `core` category"},{"metadata":{},"cell_type":"markdown","source":"In this section, we'll just plot the size of the messages classified in the _core_ category.\n\nTo do that, we'll need to filter using the first type of the `RDD[(List[String], Email)]`, the list of categories for each email. That just means that we need to check that `\"core\"` is present.\n\nThen we'll retain only the size of the email's text, we simply map the size."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val coreSizes = cats.filter(_._1 contains \"core\").map(_._2.text.size)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Still, we cannot exploit this result, since we kept being in the lazy world of `RDD`.\n\nTo plot the real data, which requires _to call the computation_, we need an action. Here, we know that the dataset is reasonable in size, so that, we'll do something **really** bad: using `collect()`.\n\nThe `collect` function on `RDD` is actually calling the whole computation **AND** waiting/fetching the whole result on the local machine (the _driver_). Which means that it'd blow the memory in bigger examples cases."},{"metadata":{},"cell_type":"markdown","source":"> Note:\n>\n> Before collecting the result, we also call `zipWithIndex` then `map(_.swap)`.\n>\n> Because we'll plot a scatter plot, then the first creates tuples with the values and their indexes, the second reverses the order to have the index first and the value next."},{"metadata":{},"cell_type":"markdown","source":"**WAIT**\n\nBefore we execute it, and thus consume and process the whole data set, let's first tail the log file of this notebook!\n\n> The logs are in the `logs` folder and you can tail the last updated one like this \n>\n> ```\n>   tail -f \"$(ls -rt logs/sn-session-* | tail -n 1)\"\n> ```"},{"metadata":{},"cell_type":"markdown","source":"Now we can plot"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import notebook.front.third.wisp._\nval coreSizesGraph = coreSizes.zipWithIndex.map(_.swap).collect()\nPlot(Seq(Pairs(coreSizesGraph, \"scatter\")))","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay, we've got some points, and some are very huge apparently. Anyway, we won't explore that further and we'll stop here the analysis, since that's not the purpose of this notebook."},{"metadata":{},"cell_type":"markdown","source":"It's interesting now, to check the logs to see what has been produced...\n\n...\n\n...\n\nA lot of _reading at_ right? From where are they coming => back to the beginning of this notebook, it's in the map's function, when we parsed the json strings!\n\nWe can also check the job [in the UI](http://localhost:4040) to see why they are there."},{"metadata":{},"cell_type":"markdown","source":"The question is: **what happens if we want ot plot another category?**\n\nLet's do that!"},{"metadata":{},"cell_type":"markdown","source":"### Plot the `streaming` category"},{"metadata":{},"cell_type":"markdown","source":"Nothing new here, the code is exactly the same, we departs from `cats`, `filter`, etc."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val streamingSizes = cats.filter(_._1 contains \"streaming\").map(_._2.text.size)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And we plot after making some room in the tailed logs."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val streamingSizesGraph = streamingSizes.zipWithIndex.map(_.swap).collect()\nPlot(Seq(Pairs(coreSizesGraph, \"area\"), Pairs(streamingSizesGraph, \"area\")))","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice, we can see that streaming has less mails and, besides the huge ones in core, the sizes are more or less similar. Anyway."},{"metadata":{},"cell_type":"markdown","source":"The interesting part is in the **logs**.\n\nAgain those _reading_ everywhere. What does that mean?\n\nIt means that, while we just create a plot on another category, we went back to the data source, read it as strings, parsed to jsons, converted them to emails and classified all. Again.\n\nKeep that in a very good place in brain because we'll get back to that in next sections."},{"metadata":{},"cell_type":"markdown","source":"## Parallelism"},{"metadata":{},"cell_type":"markdown","source":"As a final word on the Spark basics, there is the **parallelism level** of computations."},{"metadata":{},"cell_type":"markdown","source":"We've regularly looked into the [Spark UI](http://localhost:4040) to check when the jobs were running, however we didn't paid attention on how they ran.\n\nLet's now tackle this part quickly. We know, that the computations have to be parallelized and this parallelization has to do with the distribution of datasets and the CPUs available.\n\nAt the begining of this notebook, we saw that the dataset is partitioned in two by Spark. But how actually are those partitions being used?\n\n`RDD` has actually the `mapPartitions` function that behaves more or less like `map` and that can help us understanding this. The difference of `mapPartitions` over `map` is the fact that the data it works on is an `Iterator` (can be lazy) on all elements from a single partition.\n\nThis access level allows us to define efficient computations on more than one element, like pre-aggregations (sum, mean) for instance.\n\nHere is an example where we compute _letter values_ (box plot's values) of each partition, then we'll check the UI to see what happens."},{"metadata":{},"cell_type":"markdown","source":"In the following code, note that `mapPartitions` takes and returns an `Iterator` allowing a dataset to be read only once in the chain of computations."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import notebook.front.third.wisp.PlotH\nimport com.quantifind.charts.highcharts._\nval letterValuesSizePerPartition = coreSizes.mapPartitions { it =>\n    val values = it.toList.sorted\n    val low    = values.head\n    val q1     = values((values.size * 0.25).toInt)\n    val median = values((values.size * 0.5).toInt)\n    val q3     = values((values.size * 0.75).toInt)\n    val high   = values.last\n    Iterator((low, q1, median, q3, high))\n} .collect()\n  .toSeq\n  .map{ case (low:Int, q1:Int, median:Int, q3:Int, high:Int) => \n    BoxplotData(None, low, q1, median, q3, high)\n  }\nPlotH(Highchart(Seq(Series(letterValuesSizePerPartition, chart=Some(\"boxplot\")))))","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We remember that the data had two partitions, this is why there is only two values in the plot.\n\nAnd the [Spark UI](http://localhost:4040) has a job whith (1 stage and) 2 tasks, the whole computation took ~1 second.\n\nWhat could we do now, to understand better what those partitions are giving us (besides the distribution of course)?"},{"metadata":{},"cell_type":"markdown","source":"Let's say your machine has 8 CPUs (cores), what we can do is to use the `repartition(n:Int)` function on `RDD`.\n\nAs its name says, it will _create_ more partitions, so let's pass it `8`."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import notebook.front.third.wisp.PlotH\nimport com.quantifind.charts.highcharts._\nval letterValuesSizePerPartition = coreSizes.repartition(8).mapPartitions { \n  it =>\n    val values = it.toList.sorted\n    val low    = values.head\n    val q1     = values((values.size * 0.25).toInt)\n    val median = values((values.size * 0.5).toInt)\n    val q3     = values((values.size * 0.75).toInt)\n    val high   = values.last\n    Iterator((low, q1, median, q3, high))\n} .collect()\n  .toSeq\n  .map{ case (low:Int, q1:Int, median:Int, q3:Int, high:Int) => \n    BoxplotData(None, low, q1, median, q3, high)\n  }\nPlotH(Highchart(Seq(Series(letterValuesSizePerPartition, chart=Some(\"boxplot\")))))","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could expect that, we have now 8 values. But what do we have in the [Spark UI](http://localhost:4040)?\n\nWe have 8 tasks! And, these tasks ran all in once, in parallell!\n\nOf course, there is a price for that (scheduling) but we can dive deeper and see that each computation has an execution time reduced by a considerable factor."},{"metadata":{},"cell_type":"markdown","source":"Now, what would happen with a repartition 10?"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import notebook.front.third.wisp.PlotH\nimport com.quantifind.charts.highcharts._\nval letterValuesSizePerPartition = coreSizes.repartition(10).mapPartitions { \n  it =>\n    val values = it.toList.sorted\n    val low    = values.head\n    val q1     = values((values.size * 0.25).toInt)\n    val median = values((values.size * 0.5).toInt)\n    val q3     = values((values.size * 0.75).toInt)\n    val high   = values.last\n    Iterator((low, q1, median, q3, high))\n} .collect()\n  .toSeq\n  .map{ case (low:Int, q1:Int, median:Int, q3:Int, high:Int) => \n    BoxplotData(None, low, q1, median, q3, high)\n  }\nPlotH(Highchart(Seq(Series(letterValuesSizePerPartition, chart=Some(\"boxplot\")))))","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting, we have a skewed utilization of our CPUs, since two tasks were ran with a sensible shift in time."},{"metadata":{},"cell_type":"markdown","source":"Okay, and with a multiplier of 8, like 24?"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import notebook.front.third.wisp.PlotH\nimport com.quantifind.charts.highcharts._\nval letterValuesSizePerPartition = coreSizes.repartition(24).mapPartitions { \n  it =>\n    val values = it.toList.sorted\n    val low    = values.head\n    val q1     = values((values.size * 0.25).toInt)\n    val median = values((values.size * 0.5).toInt)\n    val q3     = values((values.size * 0.75).toInt)\n    val high   = values.last\n    Iterator((low, q1, median, q3, high))\n} .collect()\n  .toSeq\n  .map{ case (low:Int, q1:Int, median:Int, q3:Int, high:Int) => \n    BoxplotData(None, low, q1, median, q3, high)\n  }\nPlotH(Highchart(Seq(Series(letterValuesSizePerPartition, chart=Some(\"boxplot\")))))","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Spark UI shows us that the timeline has some shifts but smaller, meaning that the total delay is smaller and specilly all cores were used during the whole computations.\n\nOf course, the time is not really astonishing, the reasons are that the data is local and, of course, is tiny."}],"nbformat":4}