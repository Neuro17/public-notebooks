{"metadata":{"name":"MLlib - Planes","user_save_timestamp":"1970-01-01T01:00:00.000Z","auto_save_timestamp":"1970-01-01T01:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"customLocalRepo":null,"customRepos":null,"customDeps":null,"customImports":null,"customArgs":null,"customSparkConf":null},"cells":[{"metadata":{},"cell_type":"markdown","source":"# Using MLlib to predict planes' arrival delays"},{"metadata":{},"cell_type":"markdown","source":"We're going to play a bit with the well known _Big Dataset_ about planes trips.\n\nThe initial project is _Airline on-time performance_ and all information can be found [here](http://stat-computing.org/dataexpo/2009/)."},{"metadata":{},"cell_type":"markdown","source":"## Looking at data"},{"metadata":{},"cell_type":"markdown","source":"For this small example, we're going to use a subset of the data, that is, only the year [2008](http://stat-computing.org/dataexpo/2009/2008.csv.bz2).\n\nData can be found [here](http://stat-computing.org/dataexpo/2009/the-data.html)"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val folder = sys.env(\"DEMO_HOME\")+\"/datasets/\"","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a smaller dataset to check what is its content"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":":sh head -n 5 $folder/planes-2008-10-lines.csv","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As usual a CSV with `NA`s, lots of column, but the original is also a lot of data, hence we will use the parquet version that was generated and is available in the folder."},{"metadata":{},"cell_type":"markdown","source":"In order to load easily a parquet file, we can use the Data Source API which requires the SQL context."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val all = sqlContext.read.parquet(folder + \"/planes-2008.csv.parquet\")","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The columns description are [there](http://stat-computing.org/dataexpo/2009/the-data.html)."},{"metadata":{},"cell_type":"markdown","source":"So... how many?"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val count = all.count","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can then retrieve the list of airports"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val airports = (all.select($\"Origin\" as \"airport\") unionAll all.select($\"Dest\" as \"airport\") distinct)\n                .select(\"airport\").map(_.getAs[String](0))\n               .collect.toList","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## More exploration "},{"metadata":{},"cell_type":"markdown","source":"A simple way to check the contents of a DataFrame is to use the well known (in R) `describe` function."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"all.describe()","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the delays' distributions now"},{"metadata":{},"cell_type":"markdown","source":"First we filter the data and convert it to something easier to deal with. \n\nBoth delays will be associated to both airports, since we've have no clue they are independent."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val arrDelays = all.select($\"ArrDelay\", $\"Origin\" as \"Airport\") unionAll all.select($\"ArrDelay\", $\"Dest\" as \"Airport\")\nval depDelays = all.select($\"DepDelay\", $\"Origin\" as \"Airport\") unionAll all.select($\"DepDelay\", $\"Dest\" as \"Airport\")\n()","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also compute for all airport (origin) the difference in delays $ArrDelay - DepDelay$; which would show if they try to catch up. "},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val diffDelays = all.select($\"Origin\" as \"Airport\", ($\"ArrDelay\" - $\"DepDelay\") as \"Diff\")\n()","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compute some histograms per airport for each type of delay.\n\n**This can take ~1minute (with local[8] + 24G)**"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import org.apache.spark.sql.{DataFrame, Column}\ndef hist(d:DataFrame, c:Column)(airport:String, nbins:Int) = {\n  val (bins, counts) = d.filter($\"Airport\" === airport)\n                        .select(c)\n                        .map(_.getAs[Long](0))\n                        .histogram(nbins)\n  (bins zip counts).toList\n}\nval histArr = hist(arrDelays, $\"ArrDelay\") _\nval histDep = hist(depDelays, $\"DepDelay\") _\nval histDiff = hist(diffDelays, $\"Diff\") _","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A function to plot the delays and difference"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"def combine(airport:String, nbins:Int = 100) = \n  BarChart(histArr(airport, nbins), maxPoints = nbins) ++\n  BarChart(histDep(airport, nbins), maxPoints = nbins) ++\n  html(<h3>Difference in Delays</h3>) ++\n  BarChart(histDiff(airport, nbins), maxPoints = nbins)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot the distribution for San Francisco"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"combine(\"SFO\")","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot the distribution for Little Rock"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"combine(\"LIT\")","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning: Random Forest"},{"metadata":{},"cell_type":"markdown","source":"First we import the needed types"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"import org.apache.spark.mllib.tree.RandomForest\nimport org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.DenseVector","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature selection: let's get rid of some categorical features (for simplicity **ONLY**)"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"val keepFields =  all.schema.fields diff \n                    all.schema.fields .filter(_.dataType == org.apache.spark.sql.types.StringType)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The boring part:\n * clean the data (no impunation → if a row contains NA → discarded)\n * factorize the categorical values (Airports)\n * transform the data to the appropriate type (`LabeledPoint`)"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"val airportsBroadcast = sparkContext.broadcast(airports)\nval selected = all.na.drop()\n                  .select(\"ArrDelay\", \"Origin\" :: \"Dest\" :: (keepFields.map(_.name).toSet - \"ArrDelay\").toList:_*)\n                  .map { row =>\n                    val airports = airportsBroadcast.value\n                    val ori = airports.indexOf(row.getAs[String](1)).toDouble\n                    val dest = airports.indexOf(row.getAs[String](2)).toDouble\n                    val values = ori :: dest :: row.toSeq.toList.drop(3).map(_.toString.toDouble)\n                    LabeledPoint(row.getAs[Long](0).toDouble, new DenseVector(values.toArray))\n                  }\n()","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparation"},{"metadata":{},"cell_type":"markdown","source":"As usual, we split the dataset into training and test samples"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"// Split the data into training and test sets (30% held out for testing)\nval splits = selected.randomSplit(Array(0.7, 0.3))\nval (trainingData, testData) = (splits(0), splits(1))","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"// Train a RandomForest model.\nval categoricalFeaturesInfo = Map[Int, Int](0 → airports.size, 1 → airports.size)\nval numTrees = 3 // Use more in practice.\nval featureSubsetStrategy = \"auto\" // Let the algorithm choose.\nval impurity = \"variance\"\nval maxDepth = 4\nval maxBins = airports.size\n\nval model = RandomForest.trainRegressor(trainingData, categoricalFeaturesInfo,\n  numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"val predictions = (model.predict(testData.map(_.features)) zip testData.map(_.label))\n                    .toDF(\"prediction\", \"label\")\n                    .withColumn(\"diff\", $\"label\" - $\"prediction\")\n                    .cache()\n()","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"LineChart(predictions.select($\"diff\").limit(100))","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MSE!"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"import org.apache.spark.sql.functions._","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"predictions.select(pow($\"diff\", 2) as \"p\").agg(mean(\"p\") as \"s\").select(sqrt(\"s\")).first","outputs":[]}],"nbformat":4}