{"metadata":{"name":"Spark Streaming","user_save_timestamp":"1970-01-01T01:00:00.000Z","auto_save_timestamp":"1970-01-01T01:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"customLocalRepo":"/tmp/repo","customRepos":null,"customDeps":null,"customImports":null,"customArgs":null,"customSparkConf":{"spark.app.name":"AnomalyDetectionDemo","spark.master":"local[*]","spark.driver.memory":"10G","spark.serializer.extraDebugInfo":"true"}},"cells":[{"metadata":{},"cell_type":"markdown","source":"#Spark Use Case for Anomaly Detection"},{"metadata":{},"cell_type":"markdown","source":"#### Anomaly Detection with Apache Spark - inspired by Sean Owen's presentation https://www.youtube.com/watch?v=TC5cKYBZAeI\n\n#### Data available from: http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html"},{"metadata":{},"cell_type":"markdown","source":"## Set up"},{"metadata":{},"cell_type":"markdown","source":"### Get data"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val folder = sys.env(\"DEMO_HOME\")+\"/datasets/\"","outputs":[{"name":"stdout","output_type":"stream","text":"folder: String = /root/demo-base/datasets/\n"},{"metadata":{},"data":{"text/html":"/root/demo-base/datasets/"},"output_type":"execute_result","execution_count":1}]},{"metadata":{},"cell_type":"markdown","source":"#### The unlabeled testdata "},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val file = folder + \"kddcup.testdata.unlabeled\"","outputs":[{"name":"stdout","output_type":"stream","text":"file: String = /root/demo-base/datasets/kddcup.testdata.unlabeled\n"},{"metadata":{},"data":{"text/html":"/root/demo-base/datasets/kddcup.testdata.unlabeled"},"output_type":"execute_result","execution_count":2}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val testGz = folder+\"kddcup.testdata.unlabeled.gz\"","outputs":[{"name":"stdout","output_type":"stream","text":"testGz: String = /root/demo-base/datasets/kddcup.testdata.unlabeled.gz\n"},{"metadata":{},"data":{"text/html":"/root/demo-base/datasets/kddcup.testdata.unlabeled.gz"},"output_type":"execute_result","execution_count":3}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":":sh gunzip $testGz","outputs":[{"name":"stdout","output_type":"stream","text":"import sys.process._\nres2: String = \"\"\n"},{"metadata":{},"data":{"text/plain":""},"output_type":"execute_result","execution_count":4}]},{"metadata":{},"cell_type":"markdown","source":"#### The labeled dataset "},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val origfile = folder + \"kddcup.data\"","outputs":[{"name":"stdout","output_type":"stream","text":"origfile: String = /root/demo-base/datasets/kddcup.data\n"},{"metadata":{},"data":{"text/html":"/root/demo-base/datasets/kddcup.data"},"output_type":"execute_result","execution_count":3}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val labeledGz = folder + \"kddcup.data.gz\"","outputs":[{"name":"stdout","output_type":"stream","text":"labeledGz: String = /root/demo-base/datasets/kddcup.data.gz\n"},{"metadata":{},"data":{"text/html":"/root/demo-base/datasets/kddcup.data.gz"},"output_type":"execute_result","execution_count":10}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":":sh gunzip $labeledGz","outputs":[{"name":"stdout","output_type":"stream","text":"import sys.process._\nres5: String = \"\"\n"},{"metadata":{},"data":{"text/plain":""},"output_type":"execute_result","execution_count":11}]},{"metadata":{},"cell_type":"markdown","source":"### Create json version of the data"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val sqlContext = new org.apache.spark.sql.SQLContext(sparkContext)\nimport sqlContext.implicits._","outputs":[{"name":"stdout","output_type":"stream","text":"sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@547b1934\nimport sqlContext.implicits._\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":4}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\nval headers =  List(\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"label\")\nval stringRange = (headers.size-1) :: (1 to 3).toList\nval schema = StructType(headers.zipWithIndex.map {\n  case (n, i) if stringRange contains i => StructField(n, StringType, true)\n  case (n, _)                           => StructField(n, DoubleType, true)\n})\nval rows = sparkContext.textFile(origfile).map(_.split(\",\").zipWithIndex.map { \n  case (v, i) if stringRange contains i => v\n  case (v, _) => v.toDouble\n}).map(xs => Row(xs:_*))\nval jsonData = sqlContext.createDataFrame(rows, schema)\n()","outputs":[{"name":"stdout","output_type":"stream","text":"import org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\nheaders: List[String] = List(duration, protocol_type, service, flag, src_bytes, dst_bytes, land, wrong_fragment, urgent, hot, num_failed_logins, logged_in, num_compromised, root_shell, su_attempted, num_root, num_file_creations, num_shells, num_access_files, num_outbound_cmds, is_host_login, is_guest_login, count, srv_count, serror_rate, srv_serror_rate, rerror_rate, srv_rerror_rate, same_srv_rate, diff_srv_rate, srv_diff_host_rate, dst_host_count, dst_host_srv_count, dst_host_same_srv_rate, dst_host_diff_srv_rate, dst_host_same_src_port_rate, dst_host_srv_diff_host_rate, dst_host_serror_rate, dst_host_srv_serror_rate, dst_host_rerror_rate, dst_host_srv_rerror_rate, label)\nstringRange: List[Int] = List(41, 1, 2, 3)\nsche..."},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":5}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"jsonData.sample(false, 0.001)","outputs":[{"name":"stdout","output_type":"stream","text":"res3: org.apache.spark.sql.DataFrame = [duration: double, protocol_type: string, service: string, flag: string, src_bytes: double, dst_bytes: double, land: double, wrong_fragment: double, urgent: double, hot: double, num_failed_logins: double, logged_in: double, num_compromised: double, root_shell: double, su_attempted: double, num_root: double, num_file_creations: double, num_shells: double, num_access_files: double, num_outbound_cmds: double, is_host_login: double, is_guest_login: double, count: double, srv_count: double, serror_rate: double, srv_serror_rate: double, rerror_rate: double, srv_rerror_rate: double, same_srv_rate: double, diff_srv_rate: double, srv_diff_host_rate: double, dst_host_count: double, dst_host_srv_count: double, dst_host_same_srv_rate: double, dst_host_diff_srv..."},{"metadata":{},"data":{"text/html":"<div class=\"df-canvas\">\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon61cc1eaa7dee5875d5731ebbe04e2f78&quot;,&quot;partitionIndexId&quot;:&quot;anonbe509c0636afca0a7b53aa8ad3fcec37&quot;,&quot;numPartitions&quot;:195,&quot;dfSchema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;duration&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;protocol_type&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;service&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;flag&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;src_bytes&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;dst_bytes&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;land&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;wrong_fragment&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;urgent&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;hot&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;num_failed_logins&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;logged_in&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;num_compromised&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;root_shell&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;su_attempted&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;num_root&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;num_file_creations&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;num_shells&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;num_access_files&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;num_outbound_cmds&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;is_host_login&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;is_guest_login&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;count&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;srv_count&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;serror_rate&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;srv_serror_rate&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;rerror_rate&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;srv_rerror_rate&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;same_srv_rate&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;diff_srv_rate&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;srv_diff_host_rate&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;dst_host_count&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;dst_host_srv_count&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;dst_host_same_srv_rate&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;dst_host_diff_srv_rate&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;dst_host_same_src_port_rate&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;dst_host_srv_diff_host_rate&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;dst_host_serror_rate&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;dst_host_srv_serror_rate&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;dst_host_rerror_rate&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;dst_host_srv_rerror_rate&quot;,&quot;type&quot;:&quot;double&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;label&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}}]}}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/dataframe','../javascripts/notebook/consoleDir'], \n      function(dataframe, extension) {\n        dataframe.call(data, this, extension);\n      }\n    );/*]]>*/</script>\n      <link rel=\"stylesheet\" href=\"/assets/stylesheets/ipython/css/dataframe.css\" type=\"text/css\"/>\n    </div>"},"output_type":"execute_result","execution_count":6}]},{"metadata":{},"cell_type":"markdown","source":"## Writing back as JSON"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"jsonData.write.json(\"/tmp/kddcup.data.json\")","outputs":[{"name":"stdout","output_type":"stream","text":"org.apache.spark.sql.AnalysisException: path file:/tmp/kddcup.data.json already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation.run(InsertIntoHadoopFsRelation.scala:76)\n\tat org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)\n\tat org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)\n\tat org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:69)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:933)\n\tat org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:933)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:197)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:146)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:137)\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:293)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:71)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:76)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:78)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:80)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:82)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:84)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:86)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:88)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:90)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:92)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:94)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:96)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:98)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:100)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:102)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:104)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:106)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:108)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:110)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:112)\n\tat $iwC$$iwC$$iwC$$iwC.<init>(<console>:114)\n\tat $iwC$$iwC$$iwC.<init>(<console>:116)\n\tat $iwC$$iwC.<init>(<console>:118)\n\tat $iwC.<init>(<console>:120)\n\tat <init>(<console>:122)\n\tat .<init>(<console>:126)\n\tat .<clinit>(<console>)\n\tat .<init>(<console>:7)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat notebook.kernel.Repl$$anonfun$3.apply(Repl.scala:173)\n\tat notebook.kernel.Repl$$anonfun$3.apply(Repl.scala:173)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat scala.Console$.withOut(Console.scala:126)\n\tat notebook.kernel.Repl.evaluate(Repl.scala:172)\n\tat notebook.client.ReplCalculator$$anonfun$11$$anon$1$$anonfun$25.apply(ReplCalculator.scala:367)\n\tat notebook.client.ReplCalculator$$anonfun$11$$anon$1$$anonfun$25.apply(ReplCalculator.scala:364)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n\n"}]},{"metadata":{},"cell_type":"markdown","source":"### All the imports"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import org.apache.spark.mllib.clustering._\nimport org.apache.spark.mllib.linalg._\nimport org.apache.spark.mllib.feature.StandardScaler\nimport org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.streaming._\nimport org.apache.spark.Logging\nimport org.apache.log4j.{Level, Logger}","outputs":[{"name":"stdout","output_type":"stream","text":"import org.apache.spark.mllib.clustering._\nimport org.apache.spark.mllib.linalg._\nimport org.apache.spark.mllib.feature.StandardScaler\nimport org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.streaming._\nimport org.apache.spark.Logging\nimport org.apache.log4j.{Level, Logger}\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":8}]},{"metadata":{},"cell_type":"markdown","source":"***\n## Reading in and exploring the JSON data"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val dataFrame = sqlContext.read.json(\"/tmp/kddcup.data.json\").cache\n()","outputs":[{"name":"stdout","output_type":"stream","text":"dataFrame: org.apache.spark.sql.DataFrame = [count: double, diff_srv_rate: double, dst_bytes: double, dst_host_count: double, dst_host_diff_srv_rate: double, dst_host_rerror_rate: double, dst_host_same_src_port_rate: double, dst_host_same_srv_rate: double, dst_host_serror_rate: double, dst_host_srv_count: double, dst_host_srv_diff_host_rate: double, dst_host_srv_rerror_rate: double, dst_host_srv_serror_rate: double, duration: double, flag: string, hot: double, is_guest_login: double, is_host_login: double, label: string, land: double, logged_in: double, num_access_files: double, num_compromised: double, num_failed_logins: double, num_file_creations: double, num_outbound_cmds: double, num_root: double, num_shells: double, protocol_type: string, rerror_rate: double, root_shell: double, sa..."},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":9}]},{"metadata":{},"cell_type":"markdown","source":"#### There are nearly 5 million records"},{"metadata":{},"cell_type":"markdown","source":"Here we read as json, count and put in cache."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"dataFrame.count","outputs":[{"name":"stdout","output_type":"stream","text":"res6: Long = 4898431\n"},{"metadata":{},"data":{"text/html":"4898431"},"output_type":"execute_result","execution_count":10}]},{"metadata":{},"cell_type":"markdown","source":"### Let's look at the labels"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val labelsCount = dataFrame.groupBy(\"label\").count()","outputs":[{"name":"stdout","output_type":"stream","text":"labelsCount: org.apache.spark.sql.DataFrame = [label: string, count: bigint]\n"},{"metadata":{},"data":{"text/html":"<div class=\"df-canvas\">\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon8cb9daa2d728702917db3010d31cd288&quot;,&quot;partitionIndexId&quot;:&quot;anonc2a54df29ec18bc8f890dc5bcddef172&quot;,&quot;numPartitions&quot;:1,&quot;dfSchema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;label&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;count&quot;,&quot;type&quot;:&quot;long&quot;,&quot;nullable&quot;:false,&quot;metadata&quot;:{}}]}}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/dataframe','../javascripts/notebook/consoleDir'], \n      function(dataframe, extension) {\n        dataframe.call(data, this, extension);\n      }\n    );/*]]>*/</script>\n      <link rel=\"stylesheet\" href=\"/assets/stylesheets/ipython/css/dataframe.css\" type=\"text/css\"/>\n    </div>"},"output_type":"execute_result","execution_count":11}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"PieChart(labelsCount.orderBy($\"count\" desc), maxPoints=50)","outputs":[{"name":"stdout","output_type":"stream","text":"warning: there were 1 feature warning(s); re-run with -feature for details\nres7: notebook.front.widgets.PieChart[org.apache.spark.sql.DataFrame] = <PieChart widget>\n"},{"metadata":{},"data":{"text/html":"<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon6949505913b4cad71a1b7c0099865a47&quot;,&quot;dataInit&quot;:[{&quot;label&quot;:&quot;smurf.&quot;,&quot;count&quot;:2807886},{&quot;label&quot;:&quot;neptune.&quot;,&quot;count&quot;:1072017},{&quot;label&quot;:&quot;normal.&quot;,&quot;count&quot;:972781},{&quot;label&quot;:&quot;satan.&quot;,&quot;count&quot;:15892},{&quot;label&quot;:&quot;ipsweep.&quot;,&quot;count&quot;:12481},{&quot;label&quot;:&quot;portsweep.&quot;,&quot;count&quot;:10413},{&quot;label&quot;:&quot;nmap.&quot;,&quot;count&quot;:2316},{&quot;label&quot;:&quot;back.&quot;,&quot;count&quot;:2203},{&quot;label&quot;:&quot;warezclient.&quot;,&quot;count&quot;:1020},{&quot;label&quot;:&quot;teardrop.&quot;,&quot;count&quot;:979},{&quot;label&quot;:&quot;pod.&quot;,&quot;count&quot;:264},{&quot;label&quot;:&quot;guess_passwd.&quot;,&quot;count&quot;:53},{&quot;label&quot;:&quot;buffer_overflow.&quot;,&quot;count&quot;:30},{&quot;label&quot;:&quot;land.&quot;,&quot;count&quot;:21},{&quot;label&quot;:&quot;warezmaster.&quot;,&quot;count&quot;:20},{&quot;label&quot;:&quot;imap.&quot;,&quot;count&quot;:12},{&quot;label&quot;:&quot;rootkit.&quot;,&quot;count&quot;:10},{&quot;label&quot;:&quot;loadmodule.&quot;,&quot;count&quot;:9},{&quot;label&quot;:&quot;ftp_write.&quot;,&quot;count&quot;:8},{&quot;label&quot;:&quot;multihop.&quot;,&quot;count&quot;:7},{&quot;label&quot;:&quot;phf.&quot;,&quot;count&quot;:4},{&quot;label&quot;:&quot;perl.&quot;,&quot;count&quot;:3},{&quot;label&quot;:&quot;spy.&quot;,&quot;count&quot;:2}],&quot;genId&quot;:&quot;8281099&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/pieChart'], \n      function(playground, _magicpieChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicpieChart,\n    \"o\": {\"series\":\"label\",\"p\":\"count\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n        <label for=\"input-anonb694c7f916df81269cca991c3941e53b\">\n      Max Points\n    </label><input id=\"input-anonb694c7f916df81269cca991c3941e53b\" type=\"number\" name=\"input-anonb694c7f916df81269cca991c3941e53b\" data-bind=\"textInput: value, fireChange: true, valueUpdate: 'input'\">\n      <script data-selector=\"#input-anonb694c7f916df81269cca991c3941e53b\" data-this=\"{&quot;valueId&quot;:&quot;anonb694c7f916df81269cca991c3941e53b&quot;,&quot;valueInit&quot;:50}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(\n ['observable', 'knockout'],\n function (Observable, ko) {\n   //console.log(\"-----------\")\n   //console.dir(this);\n   //console.dir(valueId);\n   var obs = Observable.makeObservable(valueId)\n                       .extend({ rateLimit: { //throttle\n                                   timeout: 500,\n                                   method: \"notifyWhenChangesStop\"\n                                 }\n                               }\n                       );\n   ko.applyBindings({\n     value: obs\n   }, this);\n   obs(valueInit);\n }\n)/*]]>*/</script>\n    </input>\n        <p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anonab3922277e0e59184f72bef03d11e046&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> <span style=\"color:red\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon330d6321f276c5323aa307228b04cba2&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n        <div>\n        </div>\n      </div></div>"},"output_type":"execute_result","execution_count":12}]},{"metadata":{},"cell_type":"markdown","source":"### For simplicity, selecting only non-numeric columns"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val nonNumericFrame = List()\nval labeledNumericFrame = dataFrame.drop(\"protocol_type\").drop(\"service\").drop(\"flag\").cache()\n()","outputs":[{"name":"stdout","output_type":"stream","text":"nonNumericFrame: List[Nothing] = List()\nlabeledNumericFrame: org.apache.spark.sql.DataFrame = [count: double, diff_srv_rate: double, dst_bytes: double, dst_host_count: double, dst_host_diff_srv_rate: double, dst_host_rerror_rate: double, dst_host_same_src_port_rate: double, dst_host_same_srv_rate: double, dst_host_serror_rate: double, dst_host_srv_count: double, dst_host_srv_diff_host_rate: double, dst_host_srv_rerror_rate: double, dst_host_srv_serror_rate: double, duration: double, hot: double, is_guest_login: double, is_host_login: double, label: string, land: double, logged_in: double, num_access_files: double, num_compromised: double, num_failed_logins: double, num_file_creations: double, num_outbound_cmds: double, num_root: double, num_shells: double, rerror_rate: double, root_shel..."},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":13}]},{"metadata":{},"cell_type":"markdown","source":"***\n## Build a clustering model -- Spark MLlib"},{"metadata":{},"cell_type":"markdown","source":"### Prepare the data"},{"metadata":{},"cell_type":"markdown","source":"#### Every row becomes (Label, Vector[numeric values])"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val labeledPoint = labeledNumericFrame.map{ row =>\n  val labelIndex = row.schema.fields.map(_.name).indexOf(\"label\")\n  (\n    row.getString(labelIndex), \n    Vectors.dense(row.toSeq.zipWithIndex\n                            .filterNot(_._2==labelIndex)\n                            .map(_._1)\n                            .map(s => if(s == null) 0.0 else s.toString.toDouble).toArray\n                 )\n  )\n}\n\nval rawData = labeledPoint.map(_._2).cache\n()","outputs":[{"name":"stdout","output_type":"stream","text":"labeledPoint: org.apache.spark.rdd.RDD[(String, org.apache.spark.mllib.linalg.Vector)] = MapPartitionsRDD[77] at map at <console>:73\nrawData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[78] at map at <console>:85\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":14}]},{"metadata":{},"cell_type":"markdown","source":"#### Labels are not going to be used when building the model"},{"metadata":{},"cell_type":"markdown","source":"### Scale the features and cache the results"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"val scaler = new StandardScaler().fit(rawData)\n()","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"val data = scaler.transform(rawData).cache\n()","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Use K-Means clustering"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"val numIterations = 10 //in production it should be more\nval K = 150","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"val clusters = KMeans.train(data, K, numIterations)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Meantime let's have a look at Spark UI"},{"metadata":{},"cell_type":"markdown","source":"### Now we have our model, let's apply it to the data"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"// this is a hack to workaround the serialization problems occuring while serializing a anonymous function\n// we rescope locally everything used by the function to be serialized\n// we define the function using the instances\n// we launch the computations withing the safe scope\n@transient val ser = new java.io.Serializable {\n  val lp = labeledPoint\n  val cs = clusters\n  val sc = scaler\n  val f = (x:(String, org.apache.spark.mllib.linalg.Vector)) => (cs.predict(sc.transform(x._2)), x._1)\n  val predictions = lp.map(x => f(x))\n}","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"ser.predictions","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### And let's see the clusters and their size"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"val clustersWithSize = ser.predictions.map(x => (x._1, 1)).reduceByKey((x,y) => x+y)","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"clustersWithSize.take(25).toList","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##Use case assumption:\n**All the clusters with only one point in them labeled 'normal' are fishy**"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"val clustersWithCountAndLabel = clustersWithSize.join(ser.predictions).distinct\nclustersWithCountAndLabel.take(20).toList","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"//clusters with 1 point and labeled as normal\nval suspectedAnomalousClusters = clustersWithCountAndLabel.filter(x => x._2._1 == 1 && x._2._2 == \"normal.\").map( x => x._1 )","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we have discovered the anomalous clusters"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"val anomalousClusters = suspectedAnomalousClusters.collect\nanomalousClusters.toList","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n# Start streaming some data"},{"metadata":{"trusted":true,"input_collapsed":false,"output_stream_collapsed":true,"collapsed":true},"cell_type":"code","source":":markdown\n```\ncd conf/demo\nsbt \"runMain notebook.demo.LineStreamer $file 9999\"\n```","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n## Listen to the stream of events and predict anomaly -- Spark streaming"},{"metadata":{},"cell_type":"markdown","source":"#### To create the real-time stream we will use the test dataset from the competition (in CSV format)"},{"metadata":{},"cell_type":"markdown","source":"<code>0,tcp,private,REJ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,107,6,0.00,0.00,1.00,1.00,\n0.06,0.07,0.00,255,6,0.02,0.05,0.00,0.00,0.00,0.00,1.00,1.00</code>\n"},{"metadata":{},"cell_type":"markdown","source":"### We will use a simple Java app that reads in the logs and sends them to a TCP socket"},{"metadata":{},"cell_type":"markdown","source":"### Create streaming context with batch 2s "},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"val ssc = new StreamingContext(sparkContext, Seconds(2))","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Turn down the logging"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"Logger.getRootLogger.setLevel(Level.ERROR)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Helper method for removing non-numeric columns"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"def extractNumericColumns(r: RDD[Array[String]]): RDD[Vector] = {\n  val nonNumericColumns = List(1, 2, 3)\n  r.map(s => Vectors.dense(s.filterNot(f => nonNumericColumns.contains(s.indexOf(f))).\n                           map(st => if(st == null) 0.0 else st.toDouble).toArray))\n}","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Helper method for finding the anomalies"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"def findAnomaly(r: RDD[Int], anomalousClusters: Array[Int]): RDD[String] = {\n  r.filter(x => anomalousClusters.contains(x)).map(x => \"Suspected anomaly - entry in cluster \" + x)\n}","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Start listening to the stream"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"val lines = ssc.socketTextStream(\"localhost\", 9999)","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"@transient val safeScope = new java.io.Serializable {\n  val cs = clusters\n  val sc = scaler\n  val ac = anomalousClusters\n  val compute = lines .map(x => x.split(\",\"))\n                      .transform(y => extractNumericColumns(y))\n                      .transform(x => cs.predict(sc.transform(x)))\n                      .transform(x => findAnomaly(x, ac)).print\n  \n}","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hackers and fraudsters beware!"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"ssc.start()\nssc.awaitTermination()","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"markdown","source":"#### Spark UI now is showing Streaming tab"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"ssc.stop()","outputs":[]}],"nbformat":4}