{"metadata":{"name":"Spark Fundamentals II","user_save_timestamp":"1970-01-01T01:00:00.000Z","auto_save_timestamp":"1970-01-01T01:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"customLocalRepo":null,"customRepos":null,"customDeps":null,"customImports":null,"customArgs":null,"customSparkConf":null},"cells":[{"metadata":{},"cell_type":"markdown","source":"# Fetch Data"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val dataFile = sys.env(\"DEMO_HOME\")+\"/datasets/mails-parsed.json\"","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Parse data in domain model"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import java.util.Date\nobject p extends java.io.Serializable {\n\n  case class Email(\n    id:String, \n    sender:String, \n    date:Date, \n    subject:String, \n    text:String, \n    next_url:String, \n    prev_thread:String, \n    next_thread:String\n  ) extends java.io.Serializable \n  \n  object Email extends java.io.Serializable {\n    import play.api.libs.json._\n    def fromJson(json: JsObject) = Email(\n      (json \\ \"id\").as[String], \n      (json \\ \"sender\").as[String], \n      (json \\ \"date\").as[Date], \n      (json \\ \"subject\").as[String], \n      (json \\ \"text\").as[String], \n      (json \\ \"next_url\").as[String], \n      (json \\ \"prev_thread\").as[String], \n      (json \\ \"next_thread\").as[String]    \n    )\n  }\n  \n}","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val asText = sparkContext.textFile(dataFile)\nimport play.api.libs.json._\nval asJson = asText.map { x => \n               org.apache.log4j.Logger.getLogger(\"comp\").info(\"reading at \" + new java.util.Date())\n               Json.parse(x).as[JsObject]\n            }\nval asEmail = asJson map p.Email.fromJson","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Categorize data"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val cats = asEmail.map { email =>\n  val allCategories = Map(\n    \"graphx\"        -> List(\"graphx\"),\n    \"sql\"           -> List(\"sql\",\"jdbc\"),\n    \"dataframe\"     -> List(\"dataframe\", \" df\"),\n    \"streaming\"     -> List(\"streaming\",\"kafka\",\"dstream\", \"flume\"),\n    \"core\"          -> List(\" core\", \"rdd\"),\n    \"adam\"          -> List(\"adam\"),\n    \"thunder\"       -> List(\"thunder\"),\n    \"tachyon\"       -> List(\"tachyon\"),\n    \"mllib\"         -> List(\"mllib\", \"kmeans\", \"k-means\", \"tf-idf\", \"random forest\", \" rf \", \" lda \", \" nlp \", \"knn\"),\n    \"pyspark\"       -> List(\"pyspark\",\"python\"),\n    \"sparkr\"        -> List(\"sparkr\",\" r \"),\n    \"connectors\"    -> List(\"cassandra\", \"cql\", \"couchdb\", \"kinesis\", \"neo4j\", \"riak\", \"flume\"),\n    \"cluster\"       -> List(\"yarn\", \"mesos\"),\n    \"notebook\"      -> List(\"notebook\"),\n    \"question\"      -> List(\"question\", \"?\", \" info\"),\n    \"problem\"       -> List(\"exception\", \"error\", \"issue\", \"problem\", \"doesn't\", \"does not\", \"not working\", \"unable\"),\n    \"serialization\" -> List(\"serializable\", \"serialization\", \"serializability\"),\n    \"community\"     -> List(\"unsubscribe\", \"subscribe\", \"announce\"),\n    \"build\"         -> List(\"maven\", \"sbt\", \"idea\", \"eclipse\", \" ant \", \"graddle\"),\n    \"performance\"   -> List(\"perf\", \"performance\")\n  )\n  val categories = for {\n    (cat, tokens) <- allCategories.toList if tokens.exists { token => \n                                                              (email.subject.toLowerCase contains token) || \n                                                              (email.text.toLowerCase contains token)\n                                                           }\n  } yield cat\n\n  categories.toList -> email\n}","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Filters for core and streaming"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val coreSizes = cats.filter(_._1 contains \"core\").map(_._2.text.size)","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val streamingSizes = cats.filter(_._1 contains \"streaming\").map(_._2.text.size)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Histogram: GroupBy"},{"metadata":{},"cell_type":"markdown","source":"Histogram: \n1. create bins: this can be done by iterating all elements and compute its bin index.\n2. count elements per bin: based on the bin indexes, we can count them per index."},{"metadata":{},"cell_type":"markdown","source":"## core"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"def createHistGroupBy(rdd:RDD[Int], numBins:Int) = new java.io.Serializable {\n  @transient val cs = rdd\n  val nBins = numBins\n  val max = cs.max\n  val min = cs.min\n  val binSize = (max-min).toDouble/nBins\n  val bins = { \n    val l = List.tabulate(nBins) { i => (i+1)*binSize }\n    (0d :: l.init) zip l\n  }\n  val histBars =  cs.map { i => \n                    val binIndex = (math.floor((i.toDouble-min) / binSize).toInt) min (nBins-1)\n                    (bins(binIndex), 1)\n                  }\n                  .groupBy(_._1) //This call with _._1 sounds familiar, is there something better in spark api?\n                    .mapValues{ (l:Iterable[((Double, Double), Int)]) => \n                      org.apache.log4j.Logger.getLogger(\"createHistGroupBy\").info(\"# \" + l.size)\n                      l.map(_._2).sum \n                    }\n                  .collect\n                  .toMap\n  \n  val hist = bins.map { bin =>\n    (bin, histBars.get(bin).getOrElse(0))\n  }\n  \n  @transient val plot = notebook.front.third.wisp.PlotH(\n                          com.quantifind.charts.highcharts.Histogram.histogram(\n                            hist.map { case (dd, d) =>\n                              f\"${dd._1}%.2f -> ${dd._2}%.2f\" -> d.toDouble\n                            }\n                          )\n                        )\n}","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Important:** Show \n\n```sh\ntail -n 200 -f \"`ls -rt sn-session* | tail -n 1`\" | grep createHistGroupBy\n\n```"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"createHistGroupBy(coreSizes, 10).plot","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# THIS looks like a good exercise"},{"metadata":{},"cell_type":"markdown","source":"How to have a better view on the data distribution without very long tail? Maybe a 0.95 **in between**"},{"metadata":{},"cell_type":"markdown","source":"Note, that we'll need to **sort** the data to compute the quantiles."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val scopeCore95 = new java.io.Serializable {\n  val cs = coreSizes\n  val coreSizesSorted = cs.sortBy(identity)\n  val c = coreSizesSorted.count\n  val coreSizes95 = coreSizesSorted.zipWithIndex.filter(ei => ei._2 >= 0.025*c && ei._2 <= 0.975*c).map(_._1)\n}\ncreateHistGroupBy(scopeCore95.coreSizes95, 10).plot","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Or 0.95 **from the start**"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val scopeCore95_2 = new java.io.Serializable {\n  val cs = coreSizes\n  val coreSizesSorted = cs.sortBy(identity)\n  val c = coreSizesSorted.count\n  val coreSizes95 = coreSizesSorted.zipWithIndex.filter(ei => ei._2 <= 0.95*c).map(_._1)\n}\ncreateHistGroupBy(scopeCore95_2.coreSizes95, 10).plot","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again an example why more data is generally better (2 modes, smooth start in lowest values)"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"createHistGroupBy(scopeCore95_2.coreSizes95, 100).plot","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check with this dummy test:"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"createHistGroupBy(scopeCore95_2.coreSizes95, 1).plot","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's go to the [Spark UI](http://localhost:4040) and look into the `collect` job.\n\nIt is composed of **three stages**, a stage is decomposition of the job."},{"metadata":{},"cell_type":"markdown","source":"# Stage"},{"metadata":{},"cell_type":"markdown","source":"Back to the Spark UI, we have several stages that composed the creation of the _histogram_."},{"metadata":{},"cell_type":"markdown","source":"The stages are:\n1. sortBy\n2. groupBy\n3. collect"},{"metadata":{},"cell_type":"markdown","source":"The code flow was like this:\n* `coreSizes` is used to compute the histogram, which is computed **by email**\n* `scopeCore95_2` is essentially **sorting** the data to compute quantiles\n* `createHistGroupBy` will **rearrange** the data by bin\n* as a final step, plotting, we **collect** the data locally"},{"metadata":{},"cell_type":"markdown","source":"However, there are a lot of intermediate steps that we just ignored, and the reason why is simple:\n\n> all steps that can work locally on the same original data are part of the same stage"},{"metadata":{},"cell_type":"markdown","source":"For instance, we decoupled `coreSizes` and `scopeCore95_2` but we said that `coreSizes` is computed elements one by one, altough **sorting** will require some wrestling with the **distributed** data."},{"metadata":{},"cell_type":"markdown","source":"That is, to sort we need to know the **global order** we need some synchronization between all data nodes."},{"metadata":{},"cell_type":"markdown","source":"We can schematize it like this (it's **not** done like this):"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val part_0 = List(3, 1, 2)\nval part_1 = List(2, 4)\n\nval (n_part_0, n_part_1) = (part_0 ::: part_1).sorted      // We GATHER the data \n                                              .splitAt(3)  // we REPARTITION the data\n()","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That means that we need to **shuflle** the data around in the cluster to construct this global index."},{"metadata":{},"cell_type":"markdown","source":"However, operations like `map` and `filter` don't require that, moreover their **chaining** neither!\n\nSo such chain can work on a stream idenpendently, such stream is then a partition."},{"metadata":{},"cell_type":"markdown","source":"One reason why we can package the operations based on _shuffle or not shuffle_ is the fact that a chain of _non shuffling_ operations can be thought as atomic, and thus good candidates for milestones."},{"metadata":{},"cell_type":"markdown","source":"Hence, the worflow is first cut at `sort`.\n\nThe second cut is at `groupBy`. Of course, grouping data needs also to have communication between nodes because a **key** will probably have **values** in different nodes.\n\nThe last one is trivial, `collect`, needs to serialize the results back to the driver."},{"metadata":{},"cell_type":"markdown","source":"For the record, the _shuffle or non shuffle_ in the Spark linguo is named `Dependencies`. So there are essentially:\n* `OneToOneDependency`\n* `ShuffleDependency`"},{"metadata":{},"cell_type":"markdown","source":"# Metrics"},{"metadata":{},"cell_type":"markdown","source":"We're talking about distributed data, and even potentially, big datasets.\n\nSo, using the API is fine but cannot be enough. The Spark API works with behavior, however, the same behavior can be described differently. The choices need to be made onto metrics like time gain, resources optimization and so on.\n\nOne of the factor that is worth paying attention is the **shuffling** metrics."},{"metadata":{},"cell_type":"markdown","source":"That is, how much data, or specially how big the data, has to be serialized and sent around in the cluster to accomplish a task."},{"metadata":{},"cell_type":"markdown","source":"Checking this out is very simple thanks to the [Spark UI](http://localhost:4040), in the `collect` job, we select the `groupBy` stage.\n\nPreviously, we saw in the logs that the list was _huge_ (let's say) in memory, this has two main impacts on the performances that we can check on this page:\n* the **GC** for each task is important\n* the **write** size is also important (up to `25%` of a partition size)"},{"metadata":{},"cell_type":"markdown","source":"## GC"},{"metadata":{},"cell_type":"markdown","source":"The `groupBy` is effectively reorganizing the data by key value pairs where the value is the full list of values of the key. Which means shuffling all data to a chosen node for each key.\n\nIn the last plot we created, one node was chosen and the **full** dataset was sent to it.\n\nSince it can be big, and thus the data cannot be hold in memory, and thus the GC can go nuts."},{"metadata":{},"cell_type":"markdown","source":"## Write"},{"metadata":{},"cell_type":"markdown","source":"So we sent all original data to a single node, then only the count is done! Consequences of what, we jsut have shuffle terabytes of data to issue only one long value.\n\nSo there should be better ways to accomplish this task. We'll see that in a few."},{"metadata":{},"cell_type":"markdown","source":"# Streaming"},{"metadata":{},"cell_type":"markdown","source":"So, now we need to analyse `core` and `streaming` separately for now. So we can depart from the initial filtered data `streamingSizes`."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"createHistGroupBy(streamingSizes, 10).plot","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the UI, we see shuffled data, and probably the same data was shuffled twice specially when categories are cooccuring like Streaming mails have a great potential to be tagged core ones too."},{"metadata":{},"cell_type":"markdown","source":"So we need to see how we can optimize this."},{"metadata":{},"cell_type":"markdown","source":"# Histogram: use partitions to maximise local process"},{"metadata":{},"cell_type":"markdown","source":"A first optimization is to pre-compute as much as possible locally on the nodes and minimise the size of the data to be shuffled data."},{"metadata":{},"cell_type":"markdown","source":"Using:\n* keyBy\n* mapPartitions\n* groupByKey ~ groupBy(_._1)"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"def createHistByPartition(rdd:RDD[Int], numBins:Int) = new java.io.Serializable {\n  @transient val cs = rdd\n  val nBins = numBins\n  val max = cs.max\n  val min = cs.min\n  val binSize = (max-min).toDouble/nBins\n  val bins = { \n    val l = List.tabulate(nBins) { i => (i+1)*binSize }\n    (0d :: l.init) zip l\n  }\n  val histBars =  cs.keyBy { i => \n                    val binIndex = (math.floor((i.toDouble-min) / binSize).toInt) min (nBins-1)\n                    bins(binIndex)\n                  }\n                  .mapPartitions { it => // we receive a data chunck\n                    it.foldLeft(Map.empty[(Double, Double), Int]) { case (acc, (k, _)) =>\n                      val n = acc.getOrElse(k, 0) + 1 // accumulate count for k\n                      acc + (k -> n)\n                    }.toIterator\n                  } // folded the iterator of data into the serie of bin -> count \n                  .groupByKey\n                  .mapValues{ (l:Iterable[Int]) => // here the iterable only contain the partial count by partition \n                    org.apache.log4j.Logger.getLogger(\"createHistByPartition\").info(\"# \" + l.size)\n                    l.sum // we add the partial count by partition for the current bin (key)\n                  }\n                  .collect\n                  .toMap\n  \n  val hist = bins.map { bin =>\n    (bin, histBars.get(bin).getOrElse(0))\n  }\n  \n  @transient val plot = notebook.front.third.wisp.PlotH(\n                          com.quantifind.charts.highcharts.Histogram.histogram(\n                            hist.map { case (dd, d) =>\n                              f\"${dd._1}%.2f -> ${dd._2}%.2f\" -> d.toDouble\n                            }\n                          )\n                        )\n}","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"createHistByPartition(coreSizes, 10).plot","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Histogram: there are functions for that"},{"metadata":{},"cell_type":"markdown","source":"This local optimization is a well known pattern, hence the spark API defines some functions that reduces the verbosity, see [PairRDDFunctions](\nhttp://spark.apache.org/docs/1.4.0/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions):\n* `aggregateByKey` \n* `reduceByKey` \n* or `aggregate` in [RDD](http://spark.apache.org/docs/1.4.0/api/scala/index.html#org.apache.spark.rdd.RDD)\n* ..."},{"metadata":{},"cell_type":"markdown","source":"Note that the `byKey` versions are available because the notebook already contains `import org.apache.spark.SparkContext._`. \n\nThis import will actually add?*? many other functions to the `RDD` instances depending on the type of the element `T`. In this case, `T` is `(K, V)` hence it has a key value pair, and this we can enable some functions optimized to work with such data.\n\n> ?*? this is called _pimp my library_ in Scala, it's not a feature of the language that adds function in the bytecode at runtime or (like for macro) in the class defintion at compile time.\n> However, it uses the compiler awareness about the type `T` to wrap the `RDD` instance into a new instance that defines the keyed operations (in this case) working on the embed `RDD` instance."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"def createHistByPartitionSimpler(rdd:RDD[Int], numBins:Int) = new java.io.Serializable {\n  @transient val cs = rdd\n  val nBins = numBins\n  val max = cs.max\n  val min = cs.min\n  val binSize = (max-min).toDouble/nBins\n  val bins = { \n    val l = List.tabulate(nBins) { i => (i+1)*binSize }\n    (0d :: l.init) zip l\n  }\n  val histBars =  cs.keyBy { i => \n                    val binIndex = (math.floor((i.toDouble-min) / binSize).toInt) min (nBins-1)\n                    bins(binIndex)\n                  }\n                  .aggregateByKey(0)(\n                    (c, _) => c+1,//seqOp: (U, V) => U\n                    _ + _//combOp: (U, U) => U\n                  )\n                  .collect\n                  .toMap\n  \n  val hist = bins.map { bin =>\n    (bin, histBars.get(bin).getOrElse(0))\n  }\n  \n  @transient val plot = notebook.front.third.wisp.PlotH(\n                          com.quantifind.charts.highcharts.Histogram.histogram(\n                            hist.map { case (dd, d) =>\n                              f\"${dd._1}%.2f -> ${dd._2}%.2f\" -> d.toDouble\n                            }\n                          )\n                        )\n}","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"createHistByPartitionSimpler(coreSizes, 10).plot","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay, `counting` by key, then `collect` and finally `toMap` is also a common pattern since the whole data is constrained by the number of keys, and is somehow the _Hello World_ of Big Data (word count).\n\nSo, the `countByValue` is particularly handy here."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"def createHistByPartitionSimplest(rdd:RDD[Int], numBins:Int) = new java.io.Serializable {\n  @transient val cs = rdd\n  val nBins = numBins\n  val max = cs.max\n  val min = cs.min\n  val binSize = (max-min).toDouble/nBins\n  val bins = { \n    val l = List.tabulate(nBins) { i => (i+1)*binSize }\n    (0d :: l.init) zip l\n  }\n  val histBars =  cs.keyBy { i => \n                    val binIndex = (math.floor((i.toDouble-min) / binSize).toInt) min (nBins-1)\n                    bins(binIndex)\n                  }\n                  .countByKey\n  \n  val hist = bins.map { bin =>\n    (bin, histBars.get(bin).getOrElse(0L))\n  }\n  \n  @transient val plot = notebook.front.third.wisp.PlotH(\n                          com.quantifind.charts.highcharts.Histogram.histogram(\n                            hist.map { case (dd, d) =>\n                              f\"${dd._1}%.2f -> ${dd._2}%.2f\" -> d.toDouble\n                            }\n                          )\n                        )\n}","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"createHistByPartitionSimplest(coreSizes, 10).plot","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DR<small>edo</small>Y"},{"metadata":{},"cell_type":"markdown","source":"## Analyse `core` and `streaming`"},{"metadata":{"trusted":true,"input_collapsed":false,"output_stream_collapsed":true,"collapsed":false},"cell_type":"code","source":"val js = \"\"\"\nfunction(data, headers, chart) {\n  chart.addCategoryAxis(\"x\", [\"_2\", \"_1\"]);\n  chart.addMeasureAxis(\"y\", \"_3\");\n  chart.addSeries(\"_1\", dimple.plot.bar);\n  chart.addLegend(60, 10, 500, 20, \"right\");\n};\n\"\"\"\nwidgets.DiyChart(\n  createHistByPartitionSimplest(coreSizes, 10).hist.zipWithIndex.map{ case ((_, c), i) => (\"core\", \"\"+i, c)} :::\n  createHistByPartitionSimplest(streamingSizes, 10).hist.zipWithIndex.map{ case ((_, c), i) => (\"streaming\", \"\"+i, c)},\n  js,\n  maxPoints = 20\n)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The processes are looking the same, at least code-wise, so they probably can share some results, migth they be only intermediate.\n\nLuckily, there is a way to analyse that, that is, using the **DAG**, _Directed Acyclic Graph_, of computations.\n\nSo let's check the [Spark UI](http://localhost:4040) and compare the DAGs of the computations we just made."},{"metadata":{},"cell_type":"markdown","source":"## DAG: stages and resiliency"},{"metadata":{},"cell_type":"markdown","source":"We saw that a job is decomposed into **stages**. But let's think about it...\n"},{"metadata":{},"cell_type":"markdown","source":"Because it is **lazy**."},{"metadata":{},"cell_type":"markdown","source":"So when we have computations define using lazy structures. \n\nThat's one of the reasons why `RDD` implementation are definining a behavior on the data (`MappedRDD`) and it has (potentially) one or more parents.\n\nSo this structure, finally, results into a **Graph**. It is **directed** thanks to parents-children links. It has **no cycles**, because we're defining the job so you cannot define a parent based on the definition of the children."},{"metadata":{},"cell_type":"markdown","source":"Hence, thanks to the semantic of each `RDD` implementation, e.g.:\n* MappedRDD\n* ShuffledRDD\n* JdbcRDD\n* ...\n\nWe can infer what are the dependencies between operations and this package operations that can act together and split where extra communication is needed. "},{"metadata":{},"cell_type":"markdown","source":"As we said, a stage is composed of actions that can act on the same datasets, hence we can optimize **data locality**. That is the serie of computations will act on the dataset when it'll be available.\n\nThis data availability is constrained by stage, because these stages are atomic in the computation sense. So that:\n> for a given job, only one stage is executed on the cluster.\n\nThis constraint allows Spark to chain the stage efficiently, because the next stage will start only when all the data it will need are computed and available on the clusters. Thus, a stage can **pull** the data on the cluster nodes without fear since it knows that the previous stage has ended, meaning the data is complete.\n\nThat's why stages are safe at **boundaries** only, that also means that they contain **failures**."},{"metadata":{},"cell_type":"markdown","source":"Since a stage is:\n* composed of `RDD` in a DAG,\n* composed of distributed tasks,\n* work on a known dataset,\n* safe at boundaries, that is atomic,\n\nwhen a stage's task has failed, Spark can restart the samme task on **another node**:\n* that contains a replica of the needed data (like on HDFS), or\n* that can easily pull the parents' data"},{"metadata":{},"cell_type":"markdown","source":"This is another benefit of the **DAG** and the fact that a stage is self contain. Indeed, if a task fail because the node crashed, there is a high probability that this node also contained part of the data this task needed to work.\n\nHence, when the task needs to be restarted, Spark will need to somehow **reconstruct** this intermediate step, and of course it's getting worst if this intermediate step is also missing data.\n\nLuckily, the **DAG** comes to the rescue here, since dependencies are known, we know how to rebuild these intermediate steps by just **rewinding** the computations are **reapplying** them on the first available source.\n\nAgain, another point in favor of functional paradigm in distributed environment, immutability or, simply, **no side effects** are mandatory, otherwise the function applied to the data won't be idempotent, which breaks this recovery scenario.\n\nThis notion of dependencies and data/source location awareness in the cluster is generally coined as the **lineage** of a task."},{"metadata":{},"cell_type":"markdown","source":"But wait, that means that:\n* we know where are the intermediate data are;\n* a task lineage might be very long."},{"metadata":{},"cell_type":"markdown","source":"That also means that we jobs are sharing structure on the same sources, there is an opportunity to **save** some computations by asking Spark to keep them alive for more children.\n\nIf we have that save feature, that also offers us a way to shorten the recovery path by breaking the lineage at some points. Thanks to these milestones we can be very efficient in the recovery scenario too, because the number of intermediate steps between available source in the cluster and the recovered task is lower."},{"metadata":{},"cell_type":"markdown","source":"## Save computations"},{"metadata":{},"cell_type":"markdown","source":"So we would like to save some of the intermediate steps for reuse, and this has to cope with eventually big dataset.\n\nOne way to do that is to store manually the files in HDFS and so, let's see that below"},{"metadata":{},"cell_type":"markdown","source":"Effectively sharing results can be done in Spark to avoid unuseful computations, using cache."},{"metadata":{},"cell_type":"markdown","source":"### Save the pre processed data as Java objects"},{"metadata":{},"cell_type":"markdown","source":"A first interesting intermediate dataset is of course the parsed on. Since all our analyses will use the parsed version of the `Email`s rather than starting from the _json_."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val cacheRaw = s\"/tmp/emails-objects${System.currentTimeMillis}.raw\"\nasEmail.saveAsObjectFile(cacheRaw)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However, to compute `coreSizes` and `streamingSizes` there is still quite some common ground that might be helpful to save from recomputation."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val cacheCatsRaw = s\"/tmp/cats-objects${System.currentTimeMillis}.raw\"\ncats.saveAsObjectFile(cacheCatsRaw)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we've precomputed some intermediate steps and store them, we can move on the analyses."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val catsFromCache = sparkContext.objectFile[(List[String], p.Email)](s\"$cacheCatsRaw/*\")","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val coreSizesFromCache = catsFromCache.filter(_._1 contains \"core\").map(_._2.text.size)\nval streamingSizesFromCache = catsFromCache.filter(_._1 contains \"streaming\").map(_._2.text.size)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can create the histograms the `0.95` quantiles data using the less performant version, using `groupBy`."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val scopeCore95Init = new java.io.Serializable {\n  val cs = coreSizes\n  val coreSizesSorted = cs.sortBy(identity)\n  val c = coreSizesSorted.count\n  val coreSizes95 = coreSizesSorted.zipWithIndex.filter(ei => ei._2 >= 0.025*c && ei._2 <= 0.975*c).map(_._1)\n}\n\nval scopeCoreFromCache95 = new java.io.Serializable {\n  val cs = coreSizesFromCache\n  val coreSizesSorted = cs.sortBy(identity)\n  val c = coreSizesSorted.count\n  val coreSizes95 = coreSizesSorted.zipWithIndex.filter(ei => ei._2 >= 0.025*c && ei._2 <= 0.975*c).map(_._1)\n}\n\nval js = \"\"\"\nfunction(data, headers, chart) {\n  chart.addCategoryAxis(\"x\", [\"_2\", \"_1\"]);\n  chart.addMeasureAxis(\"y\", \"_3\");\n  chart.addSeries(\"_1\", dimple.plot.bar);\n  chart.addLegend(60, 10, 500, 20, \"right\");\n};\n\"\"\"\nwidgets.DiyChart(\n  createHistGroupBy(scopeCore95Init.coreSizes95, 10).hist.zipWithIndex.map{ case ((_, c), i) => (\"core\", \"\"+i, c)} :::\n  createHistGroupBy(scopeCoreFromCache95.coreSizes95, 10).hist.zipWithIndex.map{ case ((_, c), i) => (\"in cache\", \"\"+i, c)},\n  js,\n  maxPoints = 20\n)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the [Spark UI](http://localhost:4040), mainly the time to compute but also the DAG/Lineage.\n"},{"metadata":{},"cell_type":"markdown","source":"What we see in the UI is that the number of jobs has decreased from `9` to only `3` and we have a factor 10 in time performance."},{"metadata":{},"cell_type":"markdown","source":"### Use memory for intermediate data"},{"metadata":{},"cell_type":"markdown","source":"In distributed computing, optimizing a job by keeping intermediate version of the data is a common use case, however, as we discussed it might not be the best choice."},{"metadata":{},"cell_type":"markdown","source":"Hence, Spark comes with a solution for:\n* storage size\n* intermediate state eviction\n* lower the IO bound"},{"metadata":{},"cell_type":"markdown","source":"To offer that, Spark comes with this handy function `cache`.\n\nCaching in Spark has a default meaning that can be overloaded, but generally it's taken as\n> Cache data in memory\n\nConcretely, it uses the memory of all available nodes to store information in. So flavor allows it to back off the data onto disk, replicated, in serialized and so on.\n\nBut the point is that, Spark will manage this cache for you, and since it's in-memory one, the access to the cache data is as fast as possible.\n"},{"metadata":{},"cell_type":"markdown","source":"Spark will actually reserve a part of the JVM memory heap to the cache, and we some buffer, it'll evict cached partition if favor of others being used if the memory available isn't big sufficient.\n\n_Handy_, I told you!"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"asEmail.cache()\nasEmail.name = \"email-object\"","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"cats.cache()\ncats.name = \"categorized\"","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Caching needs an action in order to act, it's still a lazy one. Because if Spark can find more optimization in the further operation you'll ask on the cache data it'll include them in the cache!"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"asEmail.count\ncats.count","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val coreSizesFromMemory = cats.filter(_._1 contains \"core\").map(_._2.text.size)\nval streamingSizesFromMemory = cats.filter(_._1 contains \"streaming\").map(_._2.text.size)","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"val scopeCore95InitAgain = new java.io.Serializable {\n  val cs = coreSizes\n  val coreSizesSorted = cs.sortBy(identity)\n  val c = coreSizesSorted.count\n  val coreSizes95 = coreSizesSorted.zipWithIndex.filter(ei => ei._2 >= 0.025*c && ei._2 <= 0.975*c).map(_._1)\n}\n\nval scopeCoreFromMemory95 = new java.io.Serializable {\n  val cs = coreSizesFromMemory\n  val coreSizesSorted = cs.sortBy(identity)\n  val c = coreSizesSorted.count\n  val coreSizes95 = coreSizesSorted.zipWithIndex.filter(ei => ei._2 >= 0.025*c && ei._2 <= 0.975*c).map(_._1)\n}\n\nval js = \"\"\"\nfunction(data, headers, chart) {\n  chart.addCategoryAxis(\"x\", [\"_2\", \"_1\"]);\n  chart.addMeasureAxis(\"y\", \"_3\");\n  chart.addSeries(\"_1\", dimple.plot.bar);\n  chart.addLegend(60, 10, 500, 20, \"right\");\n};\n\"\"\"\nwidgets.DiyChart(\n  createHistGroupBy(scopeCore95InitAgain.coreSizes95, 10).hist.zipWithIndex.map{ case ((_, c), i) => (\"core\", \"\"+i, c)} :::\n  createHistGroupBy(scopeCoreFromMemory95.coreSizes95, 10).hist.zipWithIndex.map{ case ((_, c), i) => (\"in memory\", \"\"+i, c)},\n  js,\n  maxPoints = 20\n)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Actually, we didn't had to include the initial computation, since the cached RDD is in its DAG/Lineage already, hence it also benefits the cached data."}],"nbformat":4}