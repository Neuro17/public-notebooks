{"metadata":{"name":"Dicing Earthquakes","user_save_timestamp":"1970-01-01T00:00:00.000Z","auto_save_timestamp":"1970-01-01T00:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"customLocalRepo":"/root/.ivy2","customRepos":null,"customDeps":["com.datastax.spark:spark-cassandra-connector_2.10:1.5.0-M1","org.apache.spark:spark-streaming-kafka_2.10:1.5.0","org.apache.spark % spark-graphx_2.10 % 1.5.0","- org.apache.spark % spark-core_2.10 % _","- org.apache.spark % spark-streaming_2.10 % _","- org.apache.hadoop % _ % _"],"customImports":null,"customArgs":null,"customSparkConf":{"spark.executor.memory":"1024m","spark.cassandra.connection.host":"127.0.0.1","spark.cores.max":"2","spark.executor.cores":"2","spark.master":"local[*]","spark.scheduler.mode":"FAIR"}},"cells":[{"metadata":{},"cell_type":"markdown","source":"# Dicing Earthquakes"},{"metadata":{},"cell_type":"markdown","source":"## Create views of (Spark Steaming) incoming Earthquakes (Kafka) <br> And materialize (Cassandra) them"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, we're going to focus on:\n1. reading and processing static data about earthquakes\n2. using data frame to shape them, organize them so we can do simple exploration\n3. use the time of occurence of the earthquakes to simulate a stream in a kafka topic\n4. read the data back using spark streamin to check its content looks good"},{"metadata":{},"cell_type":"markdown","source":"# Set Up"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"import org.apache.spark.streaming.Seconds\nimport org.apache.spark.streaming.StreamingContext\nStreamingContext.getActive foreach (_.stop(false))","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"output_stream_collapsed":true,"collapsed":true},"cell_type":"code","source":"@transient val ssc = new StreamingContext(sc, Seconds(2))\nval brokers = \"localhost:9092,localhost:9093\"\nval topics = Set(\"demo\")\nval kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers, \"auto.offset.reset\" -> \"largest\" /*\"smallest\"*/)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Connect to Kafka"},{"metadata":{},"cell_type":"markdown","source":"It's finally time to connect to kafka."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"import org.apache.spark.streaming.kafka.KafkaUtils\nimport org.apache.spark.streaming.Time\nimport kafka.serializer.StringDecoder\n\ntype SD = StringDecoder\n@transient val eqStream = KafkaUtils.createDirectStream[String, String, SD, SD](ssc, kafkaParams, topics)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Views"},{"metadata":{},"cell_type":"markdown","source":"Now it's time to create some views over the data we just received in Cassandra.\n\nWe'll target two kind of analysis, hence two tables:\n* Time oriented projection: for time based analyses\n* Geo oriented projection: for geo contextual (or comparison) analyses."},{"metadata":{},"cell_type":"markdown","source":"## Create Ouput sinks"},{"metadata":{},"cell_type":"markdown","source":"Let's create these views in Cassandra then, using the <a href=\"https://github.com/datastax/spark-cassandra-connector/\" target=\"_blank\"><strong>Datastax's Cassandra connector for Spark</strong></a>."},{"metadata":{},"cell_type":"markdown","source":"First, the view for the time oriented analyses."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"import com.datastax.spark.connector.cql.CassandraConnector\n\nCassandraConnector(sparkContext.getConf).withSessionDo { session =>\n  session.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS demo.years (\n      batchtime bigint,\n      year int, \n      sum double, \n      avg int,\n      PRIMARY KEY(year, batchtime)\n    )\n    \"\"\"\n  )\n}","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And then, the view for the geospatial ones."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"import com.datastax.spark.connector.cql.CassandraConnector\n\nCassandraConnector(sparkContext.getConf).withSessionDo { session =>\n  session.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS demo.geobin (\n      batchtime bigint,\n      latbin int, \n      lonbin int, \n      year int, \n      sum double, \n      avg int,\n      PRIMARY KEY((latbin, lonbin), batchtime)\n    )\"\"\"\n  )\n}","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate Outputs"},{"metadata":{},"cell_type":"markdown","source":"Since we have a stream of data incoming a a dedicated topic in Kafka, best is, of course, to use Spark Streaming combined with the DataStax connector to dump data as they come in but aggregated per batches."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"val sqlContext = new org.apache.spark.sql.SQLContext(sparkContext)\nimport sqlContext.implicits._","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"import org.apache.spark.streaming.kafka.KafkaUtils\nimport org.apache.spark.streaming.Time\nimport kafka.serializer.StringDecoder\n\nimport org.apache.spark.sql.functions._\n\nval scope = new java.io.Serializable {\n  @transient val kp = kafkaParams\n  @transient val ts = topics\n\n  @transient val eqs = eqStream \n  \n  // transform the CSV Strings into GeoShake\n  val shakes = eqs.foreachRDD { (message: RDD[(String, String)], batchTime: Time) => \n    // convert each RDD from the batch into a DataFrame\n    val df = message.map(_._2)\n                     .map{ s => \n                        val List(year, latBin, lonBin, avg, sum) = s.split(\",\").toList\n                                                                    .map(s => Option(s)\n                                                                                .filter(_ != \"null\")\n                                                                                .map(_.toDouble)\n                                                                        )\n                        (year, latBin, lonBin, avg, sum)\n                     }\n                    .toDF(\"year\", \"latbin\", \"lonbin\", \"isum\", \"iavg\")\n\n    val timeView = df.groupBy(\"year\")\n                       .agg(sum(\"isum\").as(\"sum\"), avg(\"iavg\").as(\"avg\"))\n                         .withColumn(\"batchtime\", org.apache.spark.sql.functions.lit(batchTime.milliseconds))\n    timeView.write.format(\"org.apache.spark.sql.cassandra\")\n      .mode(org.apache.spark.sql.SaveMode.Append)\n      .options(Map(\"keyspace\" -> \"demo\", \"table\" -> \"years\"))\n      .save()\n                               \n                               \n    // CREATING Geo oriented view\n    val geoView = df.groupBy(\"latbin\", \"lonbin\", \"year\")\n                       .agg(sum(\"isum\").as(\"sum\"), avg(\"iavg\").as(\"avg\"))\n                         .withColumn(\"batchtime\", org.apache.spark.sql.functions.lit(batchTime.milliseconds))\n    geoView.write.format(\"org.apache.spark.sql.cassandra\")\n      .mode(org.apache.spark.sql.SaveMode.Append)\n      .options(Map(\"keyspace\" -> \"demo\", \"table\" -> \"geobin\"))\n      .save()\n    \n  }\n}","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Start Streaming"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"ssc.start","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Questions\n1. Why did we created these Cassandra tables?\n2. Is it relevant to use `sum` and `avg` as they are stored? (hint: why `batchtime`?)\n3. What solutions do we have to cope with that?\n"}],"nbformat":4}